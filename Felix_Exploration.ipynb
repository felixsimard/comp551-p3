{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Felix-Exploration.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/felixsimard/comp551-p3/blob/main/Felix_Exploration.ipynb",
      "authorship_tag": "ABX9TyNfWBPD0IyrE0Rkz0Gj5W0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p3/blob/main/Felix_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9d--OU0IF0S"
      },
      "source": [
        "# Felix's Notebook for exploring the assignment"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yNW2EISINDi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgfx5EC0kW8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dd7288-6d02-4417-9643-2b9a6725215b"
      },
      "source": [
        "# Reference: https://www.youtube.com/watch?v=pDdP0TFzsoQ\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3doYI1lAbe9"
      },
      "source": [
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5     # unnormalize\n",
        "    # npimg = img.numpy()\n",
        "    plt.imshow(img)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batbKOaCj6fa"
      },
      "source": [
        "def load_data(filename, data_path='/content/drive/MyDrive/P3-COMP551-FALL2021/'):\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    loaded_pkl = None\n",
        "    try:\n",
        "        pkl_buffered = open(data_path+''+filename,'rb')\n",
        "        loaded_pkl = pickle.load(pkl_buffered)\n",
        "    except Exception as e:\n",
        "        print(\"Error loading data: {}\".format(e))\n",
        "    return loaded_pkl\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuvaFp3CvYH-",
        "outputId": "62127257-8bd4-4b56-8f4d-abe3767fa865"
      },
      "source": [
        "# Load data\n",
        "train_l = load_data(\"images_l.pkl\")\n",
        "train_ul = load_data(\"images_ul.pkl\")\n",
        "labels_l = load_data(\"labels_l.pkl\")\n",
        "test_ul = load_data(\"images_test.pkl\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds4PQvPLM4OR"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwO8NOgHkd23"
      },
      "source": [
        "# Reference: https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDFiIo7cRWsN"
      },
      "source": [
        "# Hyper-parameters\n",
        "NUM_EPOCHS = 4\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0001"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_X-KfukA00R"
      },
      "source": [
        "# Tensor, Transform, Datasets, Dataloaders\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Tensor\n",
        "train_l_tensor = torch.Tensor(train_l)\n",
        "\n",
        "\n",
        "def labelize(lst):\n",
        "    bin_str = \"\".join(str(int(i)) for i in lst)\n",
        "    return bin_str\n",
        "\n",
        "# Labels logic\n",
        "labels_l_lst = labels_l.tolist()\n",
        "labels_l_lst = [labelize(lst) for lst in labels_l_lst]\n",
        "labels_encoder = preprocessing.LabelEncoder()\n",
        "targets = labels_encoder.fit_transform(labels_l_lst)\n",
        "labels_l_tensor = torch.as_tensor(targets)\n",
        "\n",
        "# labels_l_tensor = torch.argmax(torch.Tensor(labels_l), dim=1) # THIS IS KEY FOR THE LABELS\n",
        "\n",
        "# Datasets\n",
        "train_l_dataset = TensorDataset(train_l_tensor, labels_l_tensor)\n",
        "train_l_dataloader = DataLoader(train_l_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Test\n",
        "test_ul_tensor = torch.Tensor(test_ul)\n",
        "test_labels = torch.Tensor(np.zeros(len(test_ul)))\n",
        "test_ul_dataset = TensorDataset(test_ul_tensor, test_labels)\n",
        "test_ul_dataloader = DataLoader(test_ul_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXuPBNFHvY6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8c8472-fa56-48ee-d606-a3f810910683"
      },
      "source": [
        "# Define labels\n",
        "# Consider upper case and lower case letters?\n",
        "labels = []\n",
        "for l in range(26):\n",
        "    letter_str = [0.0 for i in range(26)]\n",
        "    letter_str[l] = 1.0\n",
        "    for d in range(10):\n",
        "        digits_str = [0.0 for j in range(10)]\n",
        "        digits_str[d] = 1.0\n",
        "        c = digits_str + letter_str\n",
        "        # c = \"\".join(c_str)\n",
        "        labels.append(c)\n",
        "print(labels[:5])\n",
        "print(len(labels))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooBvvB2DE4Vc"
      },
      "source": [
        "# Implement CONV Net\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=260):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        # self.model = torchvision.models.resnet34(pretrained=False)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 6, 5) # input channel (rgb), output channel, kernel size \n",
        "        self.pool = nn.MaxPool2d(2, 2) # define 2x2 stride for max-pooling\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # input channel size = output channel size of previous conv layer\n",
        "        self.fc1 = nn.Linear(16*11*11, 120) # fully connected layer\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)   \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) # activation function does not change size\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*11*11) # -1 tells PyTorch to infer num batches # flatten tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # no activation at end, softmax included in CrossEntropyLoss\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc2s6YwqN28x",
        "outputId": "c9fad15d-1a40-4846-d55a-3f86ff93d78b"
      },
      "source": [
        "# Test model flow\n",
        "model_test = ConvNet()\n",
        "x = torch.randn(BATCH_SIZE, 1, 56, 56)\n",
        "print(model_test(x).shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 260])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Lrt1pstW72"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-n5-iKHQxc",
        "outputId": "98a68624-38d4-4284-8301-f4281fe9c1c9"
      },
      "source": [
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # includes softmax\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "n_total_steps = len(train_l_dataloader)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_l_dataloader, 0):\n",
        "        # get inputs, data is a list of [inputs, labels]\n",
        "        inputs = data[0].to(device)[None, :]\n",
        "        inputs = inputs.permute(1, 0, 2, 3)\n",
        "        labels = data[1].to(device)\n",
        "\n",
        "        # print(\"Inputs:\", inputs.shape)\n",
        "        # print(\"labels:\", labels)\n",
        "        # print(\"labels:\", labels.shape)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 100 == 0: # print every 1000 mini-batches\n",
        "            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(train_l_dataloader)}], Loss: {loss.item():.4f}')\n",
        "            running_loss = 0.0\n",
        "        \n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [100/30000], Loss: 5.3697\n",
            "Epoch [1/4], Step [200/30000], Loss: 5.6379\n",
            "Epoch [1/4], Step [300/30000], Loss: 5.5275\n",
            "Epoch [1/4], Step [400/30000], Loss: 5.5601\n",
            "Epoch [1/4], Step [500/30000], Loss: 5.5714\n",
            "Epoch [1/4], Step [600/30000], Loss: 5.3932\n",
            "Epoch [1/4], Step [700/30000], Loss: 4.9531\n",
            "Epoch [1/4], Step [800/30000], Loss: 5.6342\n",
            "Epoch [1/4], Step [900/30000], Loss: 5.3941\n",
            "Epoch [1/4], Step [1000/30000], Loss: 5.6612\n",
            "Epoch [1/4], Step [1100/30000], Loss: 5.5638\n",
            "Epoch [1/4], Step [1200/30000], Loss: 5.7147\n",
            "Epoch [1/4], Step [1300/30000], Loss: 5.4615\n",
            "Epoch [1/4], Step [1400/30000], Loss: 5.6259\n",
            "Epoch [1/4], Step [1500/30000], Loss: 5.6782\n",
            "Epoch [1/4], Step [1600/30000], Loss: 5.7370\n",
            "Epoch [1/4], Step [1700/30000], Loss: 5.6761\n",
            "Epoch [1/4], Step [1800/30000], Loss: 5.7469\n",
            "Epoch [1/4], Step [1900/30000], Loss: 5.5963\n",
            "Epoch [1/4], Step [2000/30000], Loss: 5.6485\n",
            "Epoch [1/4], Step [2100/30000], Loss: 5.2064\n",
            "Epoch [1/4], Step [2200/30000], Loss: 5.5256\n",
            "Epoch [1/4], Step [2300/30000], Loss: 5.2456\n",
            "Epoch [1/4], Step [2400/30000], Loss: 4.3581\n",
            "Epoch [1/4], Step [2500/30000], Loss: 5.3974\n",
            "Epoch [1/4], Step [2600/30000], Loss: 5.4872\n",
            "Epoch [1/4], Step [2700/30000], Loss: 5.5097\n",
            "Epoch [1/4], Step [2800/30000], Loss: 4.9531\n",
            "Epoch [1/4], Step [2900/30000], Loss: 5.5588\n",
            "Epoch [1/4], Step [3000/30000], Loss: 5.4341\n",
            "Epoch [1/4], Step [3100/30000], Loss: 5.5593\n",
            "Epoch [1/4], Step [3200/30000], Loss: 5.6278\n",
            "Epoch [1/4], Step [3300/30000], Loss: 5.6162\n",
            "Epoch [1/4], Step [3400/30000], Loss: 5.4806\n",
            "Epoch [1/4], Step [3500/30000], Loss: 5.5236\n",
            "Epoch [1/4], Step [3600/30000], Loss: 5.5570\n",
            "Epoch [1/4], Step [3700/30000], Loss: 5.6012\n",
            "Epoch [1/4], Step [3800/30000], Loss: 5.3951\n",
            "Epoch [1/4], Step [3900/30000], Loss: 5.4914\n",
            "Epoch [1/4], Step [4000/30000], Loss: 5.6033\n",
            "Epoch [1/4], Step [4100/30000], Loss: 5.5631\n",
            "Epoch [1/4], Step [4200/30000], Loss: 4.9695\n",
            "Epoch [1/4], Step [4300/30000], Loss: 3.9774\n",
            "Epoch [1/4], Step [4400/30000], Loss: 5.4997\n",
            "Epoch [1/4], Step [4500/30000], Loss: 4.9119\n",
            "Epoch [1/4], Step [4600/30000], Loss: 5.1161\n",
            "Epoch [1/4], Step [4700/30000], Loss: 5.6904\n",
            "Epoch [1/4], Step [4800/30000], Loss: 5.5819\n",
            "Epoch [1/4], Step [4900/30000], Loss: 5.4911\n",
            "Epoch [1/4], Step [5000/30000], Loss: 5.2218\n",
            "Epoch [1/4], Step [5100/30000], Loss: 5.4447\n",
            "Epoch [1/4], Step [5200/30000], Loss: 5.5559\n",
            "Epoch [1/4], Step [5300/30000], Loss: 5.6742\n",
            "Epoch [1/4], Step [5400/30000], Loss: 5.3918\n",
            "Epoch [1/4], Step [5500/30000], Loss: 5.8285\n",
            "Epoch [1/4], Step [5600/30000], Loss: 4.6259\n",
            "Epoch [1/4], Step [5700/30000], Loss: 3.4989\n",
            "Epoch [1/4], Step [5800/30000], Loss: 4.1431\n",
            "Epoch [1/4], Step [5900/30000], Loss: 5.5437\n",
            "Epoch [1/4], Step [6000/30000], Loss: 5.7711\n",
            "Epoch [1/4], Step [6100/30000], Loss: 4.6172\n",
            "Epoch [1/4], Step [6200/30000], Loss: 5.7731\n",
            "Epoch [1/4], Step [6300/30000], Loss: 4.5634\n",
            "Epoch [1/4], Step [6400/30000], Loss: 6.1957\n",
            "Epoch [1/4], Step [6500/30000], Loss: 4.9771\n",
            "Epoch [1/4], Step [6600/30000], Loss: 5.1692\n",
            "Epoch [1/4], Step [6700/30000], Loss: 7.4845\n",
            "Epoch [1/4], Step [6800/30000], Loss: 4.0139\n",
            "Epoch [1/4], Step [6900/30000], Loss: 4.7421\n",
            "Epoch [1/4], Step [7000/30000], Loss: 4.9238\n",
            "Epoch [1/4], Step [7100/30000], Loss: 5.5011\n",
            "Epoch [1/4], Step [7200/30000], Loss: 5.6026\n",
            "Epoch [1/4], Step [7300/30000], Loss: 5.2176\n",
            "Epoch [1/4], Step [7400/30000], Loss: 4.2774\n",
            "Epoch [1/4], Step [7500/30000], Loss: 5.3071\n",
            "Epoch [1/4], Step [7600/30000], Loss: 4.5161\n",
            "Epoch [1/4], Step [7700/30000], Loss: 5.4423\n",
            "Epoch [1/4], Step [7800/30000], Loss: 5.1606\n",
            "Epoch [1/4], Step [7900/30000], Loss: 5.1323\n",
            "Epoch [1/4], Step [8000/30000], Loss: 5.3958\n",
            "Epoch [1/4], Step [8100/30000], Loss: 4.9088\n",
            "Epoch [1/4], Step [8200/30000], Loss: 5.2268\n",
            "Epoch [1/4], Step [8300/30000], Loss: 5.3534\n",
            "Epoch [1/4], Step [8400/30000], Loss: 4.2290\n",
            "Epoch [1/4], Step [8500/30000], Loss: 3.9602\n",
            "Epoch [1/4], Step [8600/30000], Loss: 5.5285\n",
            "Epoch [1/4], Step [8700/30000], Loss: 5.3895\n",
            "Epoch [1/4], Step [8800/30000], Loss: 5.4319\n",
            "Epoch [1/4], Step [8900/30000], Loss: 5.2594\n",
            "Epoch [1/4], Step [9000/30000], Loss: 5.3529\n",
            "Epoch [1/4], Step [9100/30000], Loss: 5.2471\n",
            "Epoch [1/4], Step [9200/30000], Loss: 4.9276\n",
            "Epoch [1/4], Step [9300/30000], Loss: 5.3445\n",
            "Epoch [1/4], Step [9400/30000], Loss: 4.5447\n",
            "Epoch [1/4], Step [9500/30000], Loss: 4.9855\n",
            "Epoch [1/4], Step [9600/30000], Loss: 4.9203\n",
            "Epoch [1/4], Step [9700/30000], Loss: 5.5195\n",
            "Epoch [1/4], Step [9800/30000], Loss: 5.0587\n",
            "Epoch [1/4], Step [9900/30000], Loss: 4.5108\n",
            "Epoch [1/4], Step [10000/30000], Loss: 5.3702\n",
            "Epoch [1/4], Step [10100/30000], Loss: 5.1554\n",
            "Epoch [1/4], Step [10200/30000], Loss: 5.1770\n",
            "Epoch [1/4], Step [10300/30000], Loss: 5.3971\n",
            "Epoch [1/4], Step [10400/30000], Loss: 5.5277\n",
            "Epoch [1/4], Step [10500/30000], Loss: 4.9605\n",
            "Epoch [1/4], Step [10600/30000], Loss: 4.0884\n",
            "Epoch [1/4], Step [10700/30000], Loss: 5.0944\n",
            "Epoch [1/4], Step [10800/30000], Loss: 4.8711\n",
            "Epoch [1/4], Step [10900/30000], Loss: 4.4764\n",
            "Epoch [1/4], Step [11000/30000], Loss: 5.0956\n",
            "Epoch [1/4], Step [11100/30000], Loss: 5.0999\n",
            "Epoch [1/4], Step [11200/30000], Loss: 5.5526\n",
            "Epoch [1/4], Step [11300/30000], Loss: 4.4490\n",
            "Epoch [1/4], Step [11400/30000], Loss: 4.2356\n",
            "Epoch [1/4], Step [11500/30000], Loss: 5.6010\n",
            "Epoch [1/4], Step [11600/30000], Loss: 4.8272\n",
            "Epoch [1/4], Step [11700/30000], Loss: 5.6033\n",
            "Epoch [1/4], Step [11800/30000], Loss: 5.6276\n",
            "Epoch [1/4], Step [11900/30000], Loss: 5.7144\n",
            "Epoch [1/4], Step [12000/30000], Loss: 4.2841\n",
            "Epoch [1/4], Step [12100/30000], Loss: 5.3944\n",
            "Epoch [1/4], Step [12200/30000], Loss: 5.3303\n",
            "Epoch [1/4], Step [12300/30000], Loss: 4.4702\n",
            "Epoch [1/4], Step [12400/30000], Loss: 4.0431\n",
            "Epoch [1/4], Step [12500/30000], Loss: 5.4229\n",
            "Epoch [1/4], Step [12600/30000], Loss: 4.8793\n",
            "Epoch [1/4], Step [12700/30000], Loss: 5.3046\n",
            "Epoch [1/4], Step [12800/30000], Loss: 4.3825\n",
            "Epoch [1/4], Step [12900/30000], Loss: 5.0589\n",
            "Epoch [1/4], Step [13000/30000], Loss: 4.3997\n",
            "Epoch [1/4], Step [13100/30000], Loss: 4.0378\n",
            "Epoch [1/4], Step [13200/30000], Loss: 5.3680\n",
            "Epoch [1/4], Step [13300/30000], Loss: 5.2147\n",
            "Epoch [1/4], Step [13400/30000], Loss: 4.0896\n",
            "Epoch [1/4], Step [13500/30000], Loss: 4.2057\n",
            "Epoch [1/4], Step [13600/30000], Loss: 4.6721\n",
            "Epoch [1/4], Step [13700/30000], Loss: 5.1161\n",
            "Epoch [1/4], Step [13800/30000], Loss: 4.3776\n",
            "Epoch [1/4], Step [13900/30000], Loss: 4.5509\n",
            "Epoch [1/4], Step [14000/30000], Loss: 5.4933\n",
            "Epoch [1/4], Step [14100/30000], Loss: 4.8440\n",
            "Epoch [1/4], Step [14200/30000], Loss: 5.6299\n",
            "Epoch [1/4], Step [14300/30000], Loss: 4.9392\n",
            "Epoch [1/4], Step [14400/30000], Loss: 5.2902\n",
            "Epoch [1/4], Step [14500/30000], Loss: 5.5805\n",
            "Epoch [1/4], Step [14600/30000], Loss: 4.9728\n",
            "Epoch [1/4], Step [14700/30000], Loss: 3.6210\n",
            "Epoch [1/4], Step [14800/30000], Loss: 4.9289\n",
            "Epoch [1/4], Step [14900/30000], Loss: 4.4178\n",
            "Epoch [1/4], Step [15000/30000], Loss: 6.1475\n",
            "Epoch [1/4], Step [15100/30000], Loss: 5.4454\n",
            "Epoch [1/4], Step [15200/30000], Loss: 4.0506\n",
            "Epoch [1/4], Step [15300/30000], Loss: 4.2480\n",
            "Epoch [1/4], Step [15400/30000], Loss: 4.8120\n",
            "Epoch [1/4], Step [15500/30000], Loss: 5.8276\n",
            "Epoch [1/4], Step [15600/30000], Loss: 3.9526\n",
            "Epoch [1/4], Step [15700/30000], Loss: 4.4907\n",
            "Epoch [1/4], Step [15800/30000], Loss: 5.5274\n",
            "Epoch [1/4], Step [15900/30000], Loss: 5.0774\n",
            "Epoch [1/4], Step [16000/30000], Loss: 5.1480\n",
            "Epoch [1/4], Step [16100/30000], Loss: 3.3595\n",
            "Epoch [1/4], Step [16200/30000], Loss: 4.7887\n",
            "Epoch [1/4], Step [16300/30000], Loss: 5.2707\n",
            "Epoch [1/4], Step [16400/30000], Loss: 4.7922\n",
            "Epoch [1/4], Step [16500/30000], Loss: 3.9197\n",
            "Epoch [1/4], Step [16600/30000], Loss: 4.1117\n",
            "Epoch [1/4], Step [16700/30000], Loss: 5.5969\n",
            "Epoch [1/4], Step [16800/30000], Loss: 5.3162\n",
            "Epoch [1/4], Step [16900/30000], Loss: 3.3432\n",
            "Epoch [1/4], Step [17000/30000], Loss: 5.4700\n",
            "Epoch [1/4], Step [17100/30000], Loss: 3.6225\n",
            "Epoch [1/4], Step [17200/30000], Loss: 5.1920\n",
            "Epoch [1/4], Step [17300/30000], Loss: 4.3311\n",
            "Epoch [1/4], Step [17400/30000], Loss: 4.1593\n",
            "Epoch [1/4], Step [17500/30000], Loss: 4.1187\n",
            "Epoch [1/4], Step [17600/30000], Loss: 4.4092\n",
            "Epoch [1/4], Step [17700/30000], Loss: 5.0913\n",
            "Epoch [1/4], Step [17800/30000], Loss: 3.5564\n",
            "Epoch [1/4], Step [17900/30000], Loss: 4.7740\n",
            "Epoch [1/4], Step [18000/30000], Loss: 5.1955\n",
            "Epoch [1/4], Step [18100/30000], Loss: 5.1868\n",
            "Epoch [1/4], Step [18200/30000], Loss: 4.9833\n",
            "Epoch [1/4], Step [18300/30000], Loss: 5.3413\n",
            "Epoch [1/4], Step [18400/30000], Loss: 5.3756\n",
            "Epoch [1/4], Step [18500/30000], Loss: 4.1290\n",
            "Epoch [1/4], Step [18600/30000], Loss: 4.9408\n",
            "Epoch [1/4], Step [18700/30000], Loss: 5.2715\n",
            "Epoch [1/4], Step [18800/30000], Loss: 5.1370\n",
            "Epoch [1/4], Step [18900/30000], Loss: 4.4050\n",
            "Epoch [1/4], Step [19000/30000], Loss: 4.5248\n",
            "Epoch [1/4], Step [19100/30000], Loss: 5.0825\n",
            "Epoch [1/4], Step [19200/30000], Loss: 4.6935\n",
            "Epoch [1/4], Step [19300/30000], Loss: 5.1138\n",
            "Epoch [1/4], Step [19400/30000], Loss: 4.9836\n",
            "Epoch [1/4], Step [19500/30000], Loss: 5.0675\n",
            "Epoch [1/4], Step [19600/30000], Loss: 3.8177\n",
            "Epoch [1/4], Step [19700/30000], Loss: 4.0852\n",
            "Epoch [1/4], Step [19800/30000], Loss: 5.0727\n",
            "Epoch [1/4], Step [19900/30000], Loss: 3.9399\n",
            "Epoch [1/4], Step [20000/30000], Loss: 4.9930\n",
            "Epoch [1/4], Step [20100/30000], Loss: 4.3639\n",
            "Epoch [1/4], Step [20200/30000], Loss: 4.7741\n",
            "Epoch [1/4], Step [20300/30000], Loss: 5.0002\n",
            "Epoch [1/4], Step [20400/30000], Loss: 4.1362\n",
            "Epoch [1/4], Step [20500/30000], Loss: 3.9890\n",
            "Epoch [1/4], Step [20600/30000], Loss: 4.6576\n",
            "Epoch [1/4], Step [20700/30000], Loss: 5.1706\n",
            "Epoch [1/4], Step [20800/30000], Loss: 5.8127\n",
            "Epoch [1/4], Step [20900/30000], Loss: 4.2872\n",
            "Epoch [1/4], Step [21000/30000], Loss: 4.2300\n",
            "Epoch [1/4], Step [21100/30000], Loss: 4.0377\n",
            "Epoch [1/4], Step [21200/30000], Loss: 5.8532\n",
            "Epoch [1/4], Step [21300/30000], Loss: 5.6084\n",
            "Epoch [1/4], Step [21400/30000], Loss: 5.1273\n",
            "Epoch [1/4], Step [21500/30000], Loss: 4.7543\n",
            "Epoch [1/4], Step [21600/30000], Loss: 6.5378\n",
            "Epoch [1/4], Step [21700/30000], Loss: 4.4972\n",
            "Epoch [1/4], Step [21800/30000], Loss: 3.8029\n",
            "Epoch [1/4], Step [21900/30000], Loss: 5.1809\n",
            "Epoch [1/4], Step [22000/30000], Loss: 5.2271\n",
            "Epoch [1/4], Step [22100/30000], Loss: 4.9058\n",
            "Epoch [1/4], Step [22200/30000], Loss: 4.6627\n",
            "Epoch [1/4], Step [22300/30000], Loss: 4.6762\n",
            "Epoch [1/4], Step [22400/30000], Loss: 4.5454\n",
            "Epoch [1/4], Step [22500/30000], Loss: 5.5892\n",
            "Epoch [1/4], Step [22600/30000], Loss: 4.6870\n",
            "Epoch [1/4], Step [22700/30000], Loss: 4.6724\n",
            "Epoch [1/4], Step [22800/30000], Loss: 4.3263\n",
            "Epoch [1/4], Step [22900/30000], Loss: 4.0781\n",
            "Epoch [1/4], Step [23000/30000], Loss: 5.1771\n",
            "Epoch [1/4], Step [23100/30000], Loss: 4.3496\n",
            "Epoch [1/4], Step [23200/30000], Loss: 4.5034\n",
            "Epoch [1/4], Step [23300/30000], Loss: 4.6336\n",
            "Epoch [1/4], Step [23400/30000], Loss: 5.0900\n",
            "Epoch [1/4], Step [23500/30000], Loss: 3.2933\n",
            "Epoch [1/4], Step [23600/30000], Loss: 5.3572\n",
            "Epoch [1/4], Step [23700/30000], Loss: 4.5066\n",
            "Epoch [1/4], Step [23800/30000], Loss: 5.9778\n",
            "Epoch [1/4], Step [23900/30000], Loss: 4.3705\n",
            "Epoch [1/4], Step [24000/30000], Loss: 4.3649\n",
            "Epoch [1/4], Step [24100/30000], Loss: 4.0354\n",
            "Epoch [1/4], Step [24200/30000], Loss: 3.7264\n",
            "Epoch [1/4], Step [24300/30000], Loss: 4.0646\n",
            "Epoch [1/4], Step [24400/30000], Loss: 5.0903\n",
            "Epoch [1/4], Step [24500/30000], Loss: 4.1032\n",
            "Epoch [1/4], Step [24600/30000], Loss: 5.0267\n",
            "Epoch [1/4], Step [24700/30000], Loss: 4.4510\n",
            "Epoch [1/4], Step [24800/30000], Loss: 4.2636\n",
            "Epoch [1/4], Step [24900/30000], Loss: 5.0547\n",
            "Epoch [1/4], Step [25000/30000], Loss: 4.7172\n",
            "Epoch [1/4], Step [25100/30000], Loss: 4.2385\n",
            "Epoch [1/4], Step [25200/30000], Loss: 4.5634\n",
            "Epoch [1/4], Step [25300/30000], Loss: 5.4365\n",
            "Epoch [1/4], Step [25400/30000], Loss: 5.1518\n",
            "Epoch [1/4], Step [25500/30000], Loss: 4.1193\n",
            "Epoch [1/4], Step [25600/30000], Loss: 4.5263\n",
            "Epoch [1/4], Step [25700/30000], Loss: 5.2713\n",
            "Epoch [1/4], Step [25800/30000], Loss: 4.2254\n",
            "Epoch [1/4], Step [25900/30000], Loss: 5.0738\n",
            "Epoch [1/4], Step [26000/30000], Loss: 3.7572\n",
            "Epoch [1/4], Step [26100/30000], Loss: 4.8571\n",
            "Epoch [1/4], Step [26200/30000], Loss: 4.6027\n",
            "Epoch [1/4], Step [26300/30000], Loss: 4.8244\n",
            "Epoch [1/4], Step [26400/30000], Loss: 4.7124\n",
            "Epoch [1/4], Step [26500/30000], Loss: 4.7576\n",
            "Epoch [1/4], Step [26600/30000], Loss: 4.1854\n",
            "Epoch [1/4], Step [26700/30000], Loss: 3.4540\n",
            "Epoch [1/4], Step [26800/30000], Loss: 4.0442\n",
            "Epoch [1/4], Step [26900/30000], Loss: 4.8846\n",
            "Epoch [1/4], Step [27000/30000], Loss: 4.8131\n",
            "Epoch [1/4], Step [27100/30000], Loss: 4.7549\n",
            "Epoch [1/4], Step [27200/30000], Loss: 4.3877\n",
            "Epoch [1/4], Step [27300/30000], Loss: 5.2063\n",
            "Epoch [1/4], Step [27400/30000], Loss: 4.6198\n",
            "Epoch [1/4], Step [27500/30000], Loss: 4.0500\n",
            "Epoch [1/4], Step [27600/30000], Loss: 5.3249\n",
            "Epoch [1/4], Step [27700/30000], Loss: 4.1463\n",
            "Epoch [1/4], Step [27800/30000], Loss: 4.6687\n",
            "Epoch [1/4], Step [27900/30000], Loss: 4.5233\n",
            "Epoch [1/4], Step [28000/30000], Loss: 5.1395\n",
            "Epoch [1/4], Step [28100/30000], Loss: 4.0953\n",
            "Epoch [1/4], Step [28200/30000], Loss: 4.5312\n",
            "Epoch [1/4], Step [28300/30000], Loss: 5.0017\n",
            "Epoch [1/4], Step [28400/30000], Loss: 5.2127\n",
            "Epoch [1/4], Step [28500/30000], Loss: 4.6547\n",
            "Epoch [1/4], Step [28600/30000], Loss: 4.4731\n",
            "Epoch [1/4], Step [28700/30000], Loss: 4.9288\n",
            "Epoch [1/4], Step [28800/30000], Loss: 5.1851\n",
            "Epoch [1/4], Step [28900/30000], Loss: 5.4410\n",
            "Epoch [1/4], Step [29000/30000], Loss: 5.2522\n",
            "Epoch [1/4], Step [29100/30000], Loss: 5.3313\n",
            "Epoch [1/4], Step [29200/30000], Loss: 4.5738\n",
            "Epoch [1/4], Step [29300/30000], Loss: 5.1567\n",
            "Epoch [1/4], Step [29400/30000], Loss: 4.0074\n",
            "Epoch [1/4], Step [29500/30000], Loss: 4.4650\n",
            "Epoch [1/4], Step [29600/30000], Loss: 5.1868\n",
            "Epoch [1/4], Step [29700/30000], Loss: 4.3817\n",
            "Epoch [1/4], Step [29800/30000], Loss: 4.5834\n",
            "Epoch [1/4], Step [29900/30000], Loss: 4.9648\n",
            "Epoch [1/4], Step [30000/30000], Loss: 4.3832\n",
            "Epoch [2/4], Step [100/30000], Loss: 5.1609\n",
            "Epoch [2/4], Step [200/30000], Loss: 5.0009\n",
            "Epoch [2/4], Step [300/30000], Loss: 3.4382\n",
            "Epoch [2/4], Step [400/30000], Loss: 4.0035\n",
            "Epoch [2/4], Step [500/30000], Loss: 4.7212\n",
            "Epoch [2/4], Step [600/30000], Loss: 3.2544\n",
            "Epoch [2/4], Step [700/30000], Loss: 3.5095\n",
            "Epoch [2/4], Step [800/30000], Loss: 4.2201\n",
            "Epoch [2/4], Step [900/30000], Loss: 4.7542\n",
            "Epoch [2/4], Step [1000/30000], Loss: 4.9383\n",
            "Epoch [2/4], Step [1100/30000], Loss: 4.2756\n",
            "Epoch [2/4], Step [1200/30000], Loss: 4.3834\n",
            "Epoch [2/4], Step [1300/30000], Loss: 4.9977\n",
            "Epoch [2/4], Step [1400/30000], Loss: 5.1215\n",
            "Epoch [2/4], Step [1500/30000], Loss: 4.0966\n",
            "Epoch [2/4], Step [1600/30000], Loss: 4.5687\n",
            "Epoch [2/4], Step [1700/30000], Loss: 4.2523\n",
            "Epoch [2/4], Step [1800/30000], Loss: 5.0961\n",
            "Epoch [2/4], Step [1900/30000], Loss: 4.4109\n",
            "Epoch [2/4], Step [2000/30000], Loss: 4.1575\n",
            "Epoch [2/4], Step [2100/30000], Loss: 4.5847\n",
            "Epoch [2/4], Step [2200/30000], Loss: 4.2082\n",
            "Epoch [2/4], Step [2300/30000], Loss: 3.9854\n",
            "Epoch [2/4], Step [2400/30000], Loss: 4.1074\n",
            "Epoch [2/4], Step [2500/30000], Loss: 4.3531\n",
            "Epoch [2/4], Step [2600/30000], Loss: 4.1794\n",
            "Epoch [2/4], Step [2700/30000], Loss: 4.8960\n",
            "Epoch [2/4], Step [2800/30000], Loss: 4.6750\n",
            "Epoch [2/4], Step [2900/30000], Loss: 4.8303\n",
            "Epoch [2/4], Step [3000/30000], Loss: 3.7584\n",
            "Epoch [2/4], Step [3100/30000], Loss: 4.9882\n",
            "Epoch [2/4], Step [3200/30000], Loss: 4.0113\n",
            "Epoch [2/4], Step [3300/30000], Loss: 5.0612\n",
            "Epoch [2/4], Step [3400/30000], Loss: 4.6949\n",
            "Epoch [2/4], Step [3500/30000], Loss: 5.1848\n",
            "Epoch [2/4], Step [3600/30000], Loss: 4.7391\n",
            "Epoch [2/4], Step [3700/30000], Loss: 4.7277\n",
            "Epoch [2/4], Step [3800/30000], Loss: 4.3172\n",
            "Epoch [2/4], Step [3900/30000], Loss: 4.0422\n",
            "Epoch [2/4], Step [4000/30000], Loss: 4.7167\n",
            "Epoch [2/4], Step [4100/30000], Loss: 4.0710\n",
            "Epoch [2/4], Step [4200/30000], Loss: 4.7127\n",
            "Epoch [2/4], Step [4300/30000], Loss: 3.9837\n",
            "Epoch [2/4], Step [4400/30000], Loss: 3.9299\n",
            "Epoch [2/4], Step [4500/30000], Loss: 4.5136\n",
            "Epoch [2/4], Step [4600/30000], Loss: 4.9720\n",
            "Epoch [2/4], Step [4700/30000], Loss: 6.1168\n",
            "Epoch [2/4], Step [4800/30000], Loss: 4.1698\n",
            "Epoch [2/4], Step [4900/30000], Loss: 4.8523\n",
            "Epoch [2/4], Step [5000/30000], Loss: 4.9356\n",
            "Epoch [2/4], Step [5100/30000], Loss: 4.7405\n",
            "Epoch [2/4], Step [5200/30000], Loss: 4.9534\n",
            "Epoch [2/4], Step [5300/30000], Loss: 5.1756\n",
            "Epoch [2/4], Step [5400/30000], Loss: 4.9607\n",
            "Epoch [2/4], Step [5500/30000], Loss: 5.3175\n",
            "Epoch [2/4], Step [5600/30000], Loss: 4.3489\n",
            "Epoch [2/4], Step [5700/30000], Loss: 3.5823\n",
            "Epoch [2/4], Step [5800/30000], Loss: 4.0477\n",
            "Epoch [2/4], Step [5900/30000], Loss: 5.2918\n",
            "Epoch [2/4], Step [6000/30000], Loss: 5.2409\n",
            "Epoch [2/4], Step [6100/30000], Loss: 4.5545\n",
            "Epoch [2/4], Step [6200/30000], Loss: 4.7963\n",
            "Epoch [2/4], Step [6300/30000], Loss: 4.4429\n",
            "Epoch [2/4], Step [6400/30000], Loss: 5.3364\n",
            "Epoch [2/4], Step [6500/30000], Loss: 4.5813\n",
            "Epoch [2/4], Step [6600/30000], Loss: 4.7446\n",
            "Epoch [2/4], Step [6700/30000], Loss: 5.4953\n",
            "Epoch [2/4], Step [6800/30000], Loss: 3.9041\n",
            "Epoch [2/4], Step [6900/30000], Loss: 5.0461\n",
            "Epoch [2/4], Step [7000/30000], Loss: 4.8791\n",
            "Epoch [2/4], Step [7100/30000], Loss: 4.2663\n",
            "Epoch [2/4], Step [7200/30000], Loss: 4.7961\n",
            "Epoch [2/4], Step [7300/30000], Loss: 5.1804\n",
            "Epoch [2/4], Step [7400/30000], Loss: 4.3951\n",
            "Epoch [2/4], Step [7500/30000], Loss: 3.8200\n",
            "Epoch [2/4], Step [7600/30000], Loss: 4.3234\n",
            "Epoch [2/4], Step [7700/30000], Loss: 4.6746\n",
            "Epoch [2/4], Step [7800/30000], Loss: 5.6377\n",
            "Epoch [2/4], Step [7900/30000], Loss: 4.7911\n",
            "Epoch [2/4], Step [8000/30000], Loss: 4.5721\n",
            "Epoch [2/4], Step [8100/30000], Loss: 4.6148\n",
            "Epoch [2/4], Step [8200/30000], Loss: 4.8983\n",
            "Epoch [2/4], Step [8300/30000], Loss: 4.9609\n",
            "Epoch [2/4], Step [8400/30000], Loss: 4.0088\n",
            "Epoch [2/4], Step [8500/30000], Loss: 3.7420\n",
            "Epoch [2/4], Step [8600/30000], Loss: 4.6181\n",
            "Epoch [2/4], Step [8700/30000], Loss: 4.6867\n",
            "Epoch [2/4], Step [8800/30000], Loss: 4.4916\n",
            "Epoch [2/4], Step [8900/30000], Loss: 4.7242\n",
            "Epoch [2/4], Step [9000/30000], Loss: 4.6648\n",
            "Epoch [2/4], Step [9100/30000], Loss: 4.1051\n",
            "Epoch [2/4], Step [9200/30000], Loss: 4.7770\n",
            "Epoch [2/4], Step [9300/30000], Loss: 4.7383\n",
            "Epoch [2/4], Step [9400/30000], Loss: 4.4839\n",
            "Epoch [2/4], Step [9500/30000], Loss: 3.2650\n",
            "Epoch [2/4], Step [9600/30000], Loss: 4.6788\n",
            "Epoch [2/4], Step [9700/30000], Loss: 4.8746\n",
            "Epoch [2/4], Step [9800/30000], Loss: 4.9491\n",
            "Epoch [2/4], Step [9900/30000], Loss: 4.3570\n",
            "Epoch [2/4], Step [10000/30000], Loss: 4.6670\n",
            "Epoch [2/4], Step [10100/30000], Loss: 5.2030\n",
            "Epoch [2/4], Step [10200/30000], Loss: 4.8455\n",
            "Epoch [2/4], Step [10300/30000], Loss: 5.4496\n",
            "Epoch [2/4], Step [10400/30000], Loss: 5.6893\n",
            "Epoch [2/4], Step [10500/30000], Loss: 4.7410\n",
            "Epoch [2/4], Step [10600/30000], Loss: 4.1807\n",
            "Epoch [2/4], Step [10700/30000], Loss: 4.7876\n",
            "Epoch [2/4], Step [10800/30000], Loss: 5.2907\n",
            "Epoch [2/4], Step [10900/30000], Loss: 4.4616\n",
            "Epoch [2/4], Step [11000/30000], Loss: 4.8506\n",
            "Epoch [2/4], Step [11100/30000], Loss: 5.8727\n",
            "Epoch [2/4], Step [11200/30000], Loss: 4.5415\n",
            "Epoch [2/4], Step [11300/30000], Loss: 4.6250\n",
            "Epoch [2/4], Step [11400/30000], Loss: 3.3686\n",
            "Epoch [2/4], Step [11500/30000], Loss: 4.9597\n",
            "Epoch [2/4], Step [11600/30000], Loss: 4.7674\n",
            "Epoch [2/4], Step [11700/30000], Loss: 5.5964\n",
            "Epoch [2/4], Step [11800/30000], Loss: 5.3211\n",
            "Epoch [2/4], Step [11900/30000], Loss: 5.6498\n",
            "Epoch [2/4], Step [12000/30000], Loss: 4.1660\n",
            "Epoch [2/4], Step [12100/30000], Loss: 4.2111\n",
            "Epoch [2/4], Step [12200/30000], Loss: 5.0777\n",
            "Epoch [2/4], Step [12300/30000], Loss: 4.1526\n",
            "Epoch [2/4], Step [12400/30000], Loss: 4.1602\n",
            "Epoch [2/4], Step [12500/30000], Loss: 4.8754\n",
            "Epoch [2/4], Step [12600/30000], Loss: 4.7680\n",
            "Epoch [2/4], Step [12700/30000], Loss: 4.9347\n",
            "Epoch [2/4], Step [12800/30000], Loss: 4.1998\n",
            "Epoch [2/4], Step [12900/30000], Loss: 4.2822\n",
            "Epoch [2/4], Step [13000/30000], Loss: 3.9131\n",
            "Epoch [2/4], Step [13100/30000], Loss: 4.1104\n",
            "Epoch [2/4], Step [13200/30000], Loss: 5.8103\n",
            "Epoch [2/4], Step [13300/30000], Loss: 5.0360\n",
            "Epoch [2/4], Step [13400/30000], Loss: 4.1574\n",
            "Epoch [2/4], Step [13500/30000], Loss: 4.4536\n",
            "Epoch [2/4], Step [13600/30000], Loss: 4.7062\n",
            "Epoch [2/4], Step [13700/30000], Loss: 4.5420\n",
            "Epoch [2/4], Step [13800/30000], Loss: 4.2799\n",
            "Epoch [2/4], Step [13900/30000], Loss: 4.2555\n",
            "Epoch [2/4], Step [14000/30000], Loss: 5.1995\n",
            "Epoch [2/4], Step [14100/30000], Loss: 5.0315\n",
            "Epoch [2/4], Step [14200/30000], Loss: 6.5944\n",
            "Epoch [2/4], Step [14300/30000], Loss: 4.9714\n",
            "Epoch [2/4], Step [14400/30000], Loss: 4.8629\n",
            "Epoch [2/4], Step [14500/30000], Loss: 5.0931\n",
            "Epoch [2/4], Step [14600/30000], Loss: 5.5588\n",
            "Epoch [2/4], Step [14700/30000], Loss: 3.6312\n",
            "Epoch [2/4], Step [14800/30000], Loss: 4.2224\n",
            "Epoch [2/4], Step [14900/30000], Loss: 4.0448\n",
            "Epoch [2/4], Step [15000/30000], Loss: 5.0285\n",
            "Epoch [2/4], Step [15100/30000], Loss: 5.4310\n",
            "Epoch [2/4], Step [15200/30000], Loss: 3.8791\n",
            "Epoch [2/4], Step [15300/30000], Loss: 4.2687\n",
            "Epoch [2/4], Step [15400/30000], Loss: 5.2616\n",
            "Epoch [2/4], Step [15500/30000], Loss: 5.4368\n",
            "Epoch [2/4], Step [15600/30000], Loss: 3.9046\n",
            "Epoch [2/4], Step [15700/30000], Loss: 4.3716\n",
            "Epoch [2/4], Step [15800/30000], Loss: 6.3496\n",
            "Epoch [2/4], Step [15900/30000], Loss: 5.2453\n",
            "Epoch [2/4], Step [16000/30000], Loss: 4.6361\n",
            "Epoch [2/4], Step [16100/30000], Loss: 3.4071\n",
            "Epoch [2/4], Step [16200/30000], Loss: 4.6719\n",
            "Epoch [2/4], Step [16300/30000], Loss: 5.6629\n",
            "Epoch [2/4], Step [16400/30000], Loss: 4.5699\n",
            "Epoch [2/4], Step [16500/30000], Loss: 3.9151\n",
            "Epoch [2/4], Step [16600/30000], Loss: 4.0650\n",
            "Epoch [2/4], Step [16700/30000], Loss: 5.1656\n",
            "Epoch [2/4], Step [16800/30000], Loss: 4.8135\n",
            "Epoch [2/4], Step [16900/30000], Loss: 4.1172\n",
            "Epoch [2/4], Step [17000/30000], Loss: 5.5291\n",
            "Epoch [2/4], Step [17100/30000], Loss: 3.6617\n",
            "Epoch [2/4], Step [17200/30000], Loss: 4.9187\n",
            "Epoch [2/4], Step [17300/30000], Loss: 4.7813\n",
            "Epoch [2/4], Step [17400/30000], Loss: 4.2795\n",
            "Epoch [2/4], Step [17500/30000], Loss: 4.0741\n",
            "Epoch [2/4], Step [17600/30000], Loss: 4.0073\n",
            "Epoch [2/4], Step [17700/30000], Loss: 4.5594\n",
            "Epoch [2/4], Step [17800/30000], Loss: 3.5736\n",
            "Epoch [2/4], Step [17900/30000], Loss: 4.7120\n",
            "Epoch [2/4], Step [18000/30000], Loss: 5.1319\n",
            "Epoch [2/4], Step [18100/30000], Loss: 4.7377\n",
            "Epoch [2/4], Step [18200/30000], Loss: 5.1305\n",
            "Epoch [2/4], Step [18300/30000], Loss: 4.9240\n",
            "Epoch [2/4], Step [18400/30000], Loss: 4.7921\n",
            "Epoch [2/4], Step [18500/30000], Loss: 4.0407\n",
            "Epoch [2/4], Step [18600/30000], Loss: 4.1623\n",
            "Epoch [2/4], Step [18700/30000], Loss: 4.9615\n",
            "Epoch [2/4], Step [18800/30000], Loss: 4.7103\n",
            "Epoch [2/4], Step [18900/30000], Loss: 4.3770\n",
            "Epoch [2/4], Step [19000/30000], Loss: 4.3744\n",
            "Epoch [2/4], Step [19100/30000], Loss: 4.5741\n",
            "Epoch [2/4], Step [19200/30000], Loss: 4.5825\n",
            "Epoch [2/4], Step [19300/30000], Loss: 4.7249\n",
            "Epoch [2/4], Step [19400/30000], Loss: 4.9622\n",
            "Epoch [2/4], Step [19500/30000], Loss: 4.8737\n",
            "Epoch [2/4], Step [19600/30000], Loss: 3.8185\n",
            "Epoch [2/4], Step [19700/30000], Loss: 4.1435\n",
            "Epoch [2/4], Step [19800/30000], Loss: 4.8302\n",
            "Epoch [2/4], Step [19900/30000], Loss: 4.0642\n",
            "Epoch [2/4], Step [20000/30000], Loss: 4.9177\n",
            "Epoch [2/4], Step [20100/30000], Loss: 4.2313\n",
            "Epoch [2/4], Step [20200/30000], Loss: 4.4135\n",
            "Epoch [2/4], Step [20300/30000], Loss: 4.4726\n",
            "Epoch [2/4], Step [20400/30000], Loss: 4.1630\n",
            "Epoch [2/4], Step [20500/30000], Loss: 4.2970\n",
            "Epoch [2/4], Step [20600/30000], Loss: 5.4295\n",
            "Epoch [2/4], Step [20700/30000], Loss: 5.2492\n",
            "Epoch [2/4], Step [20800/30000], Loss: 5.3057\n",
            "Epoch [2/4], Step [20900/30000], Loss: 4.3443\n",
            "Epoch [2/4], Step [21000/30000], Loss: 4.2088\n",
            "Epoch [2/4], Step [21100/30000], Loss: 3.9440\n",
            "Epoch [2/4], Step [21200/30000], Loss: 6.1112\n",
            "Epoch [2/4], Step [21300/30000], Loss: 5.6594\n",
            "Epoch [2/4], Step [21400/30000], Loss: 4.8729\n",
            "Epoch [2/4], Step [21500/30000], Loss: 4.9443\n",
            "Epoch [2/4], Step [21600/30000], Loss: 5.2480\n",
            "Epoch [2/4], Step [21700/30000], Loss: 4.3883\n",
            "Epoch [2/4], Step [21800/30000], Loss: 3.8834\n",
            "Epoch [2/4], Step [21900/30000], Loss: 4.9381\n",
            "Epoch [2/4], Step [22000/30000], Loss: 4.6335\n",
            "Epoch [2/4], Step [22100/30000], Loss: 5.3419\n",
            "Epoch [2/4], Step [22200/30000], Loss: 4.8598\n",
            "Epoch [2/4], Step [22300/30000], Loss: 4.6688\n",
            "Epoch [2/4], Step [22400/30000], Loss: 4.4468\n",
            "Epoch [2/4], Step [22500/30000], Loss: 5.4705\n",
            "Epoch [2/4], Step [22600/30000], Loss: 4.5517\n",
            "Epoch [2/4], Step [22700/30000], Loss: 4.8417\n",
            "Epoch [2/4], Step [22800/30000], Loss: 4.3521\n",
            "Epoch [2/4], Step [22900/30000], Loss: 4.0857\n",
            "Epoch [2/4], Step [23000/30000], Loss: 4.8433\n",
            "Epoch [2/4], Step [23100/30000], Loss: 4.3173\n",
            "Epoch [2/4], Step [23200/30000], Loss: 4.3928\n",
            "Epoch [2/4], Step [23300/30000], Loss: 4.4799\n",
            "Epoch [2/4], Step [23400/30000], Loss: 4.9637\n",
            "Epoch [2/4], Step [23500/30000], Loss: 3.2964\n",
            "Epoch [2/4], Step [23600/30000], Loss: 4.9015\n",
            "Epoch [2/4], Step [23700/30000], Loss: 4.4618\n",
            "Epoch [2/4], Step [23800/30000], Loss: 6.1469\n",
            "Epoch [2/4], Step [23900/30000], Loss: 4.8090\n",
            "Epoch [2/4], Step [24000/30000], Loss: 4.3835\n",
            "Epoch [2/4], Step [24100/30000], Loss: 4.1085\n",
            "Epoch [2/4], Step [24200/30000], Loss: 3.9208\n",
            "Epoch [2/4], Step [24300/30000], Loss: 4.0413\n",
            "Epoch [2/4], Step [24400/30000], Loss: 4.9519\n",
            "Epoch [2/4], Step [24500/30000], Loss: 4.1895\n",
            "Epoch [2/4], Step [24600/30000], Loss: 4.6703\n",
            "Epoch [2/4], Step [24700/30000], Loss: 4.1646\n",
            "Epoch [2/4], Step [24800/30000], Loss: 4.3186\n",
            "Epoch [2/4], Step [24900/30000], Loss: 4.8636\n",
            "Epoch [2/4], Step [25000/30000], Loss: 4.5118\n",
            "Epoch [2/4], Step [25100/30000], Loss: 4.2636\n",
            "Epoch [2/4], Step [25200/30000], Loss: 5.1062\n",
            "Epoch [2/4], Step [25300/30000], Loss: 5.4058\n",
            "Epoch [2/4], Step [25400/30000], Loss: 4.9602\n",
            "Epoch [2/4], Step [25500/30000], Loss: 4.0519\n",
            "Epoch [2/4], Step [25600/30000], Loss: 4.9836\n",
            "Epoch [2/4], Step [25700/30000], Loss: 5.5321\n",
            "Epoch [2/4], Step [25800/30000], Loss: 4.2426\n",
            "Epoch [2/4], Step [25900/30000], Loss: 6.0845\n",
            "Epoch [2/4], Step [26000/30000], Loss: 3.8293\n",
            "Epoch [2/4], Step [26100/30000], Loss: 5.0936\n",
            "Epoch [2/4], Step [26200/30000], Loss: 4.5073\n",
            "Epoch [2/4], Step [26300/30000], Loss: 4.6387\n",
            "Epoch [2/4], Step [26400/30000], Loss: 4.4360\n",
            "Epoch [2/4], Step [26500/30000], Loss: 4.8558\n",
            "Epoch [2/4], Step [26600/30000], Loss: 4.2089\n",
            "Epoch [2/4], Step [26700/30000], Loss: 3.6050\n",
            "Epoch [2/4], Step [26800/30000], Loss: 4.0566\n",
            "Epoch [2/4], Step [26900/30000], Loss: 4.4323\n",
            "Epoch [2/4], Step [27000/30000], Loss: 4.4118\n",
            "Epoch [2/4], Step [27100/30000], Loss: 5.6123\n",
            "Epoch [2/4], Step [27200/30000], Loss: 4.3588\n",
            "Epoch [2/4], Step [27300/30000], Loss: 5.0886\n",
            "Epoch [2/4], Step [27400/30000], Loss: 4.5488\n",
            "Epoch [2/4], Step [27500/30000], Loss: 3.9143\n",
            "Epoch [2/4], Step [27600/30000], Loss: 4.9461\n",
            "Epoch [2/4], Step [27700/30000], Loss: 4.1345\n",
            "Epoch [2/4], Step [27800/30000], Loss: 4.5583\n",
            "Epoch [2/4], Step [27900/30000], Loss: 4.4629\n",
            "Epoch [2/4], Step [28000/30000], Loss: 4.6213\n",
            "Epoch [2/4], Step [28100/30000], Loss: 4.0059\n",
            "Epoch [2/4], Step [28200/30000], Loss: 4.3919\n",
            "Epoch [2/4], Step [28300/30000], Loss: 5.0729\n",
            "Epoch [2/4], Step [28400/30000], Loss: 4.8138\n",
            "Epoch [2/4], Step [28500/30000], Loss: 4.5492\n",
            "Epoch [2/4], Step [28600/30000], Loss: 4.8574\n",
            "Epoch [2/4], Step [28700/30000], Loss: 4.7945\n",
            "Epoch [2/4], Step [28800/30000], Loss: 4.7017\n",
            "Epoch [2/4], Step [28900/30000], Loss: 5.2727\n",
            "Epoch [2/4], Step [29000/30000], Loss: 5.4773\n",
            "Epoch [2/4], Step [29100/30000], Loss: 5.3458\n",
            "Epoch [2/4], Step [29200/30000], Loss: 4.6324\n",
            "Epoch [2/4], Step [29300/30000], Loss: 5.3544\n",
            "Epoch [2/4], Step [29400/30000], Loss: 3.8540\n",
            "Epoch [2/4], Step [29500/30000], Loss: 4.3862\n",
            "Epoch [2/4], Step [29600/30000], Loss: 4.9115\n",
            "Epoch [2/4], Step [29700/30000], Loss: 4.3232\n",
            "Epoch [2/4], Step [29800/30000], Loss: 4.5970\n",
            "Epoch [2/4], Step [29900/30000], Loss: 4.5255\n",
            "Epoch [2/4], Step [30000/30000], Loss: 4.4185\n",
            "Epoch [3/4], Step [100/30000], Loss: 5.2771\n",
            "Epoch [3/4], Step [200/30000], Loss: 5.9415\n",
            "Epoch [3/4], Step [300/30000], Loss: 3.5196\n",
            "Epoch [3/4], Step [400/30000], Loss: 4.0086\n",
            "Epoch [3/4], Step [500/30000], Loss: 4.6464\n",
            "Epoch [3/4], Step [600/30000], Loss: 3.8607\n",
            "Epoch [3/4], Step [700/30000], Loss: 3.5890\n",
            "Epoch [3/4], Step [800/30000], Loss: 4.1539\n",
            "Epoch [3/4], Step [900/30000], Loss: 4.6614\n",
            "Epoch [3/4], Step [1000/30000], Loss: 5.0401\n",
            "Epoch [3/4], Step [1100/30000], Loss: 4.2289\n",
            "Epoch [3/4], Step [1200/30000], Loss: 4.2977\n",
            "Epoch [3/4], Step [1300/30000], Loss: 5.1265\n",
            "Epoch [3/4], Step [1400/30000], Loss: 5.0197\n",
            "Epoch [3/4], Step [1500/30000], Loss: 4.1082\n",
            "Epoch [3/4], Step [1600/30000], Loss: 5.6962\n",
            "Epoch [3/4], Step [1700/30000], Loss: 4.0925\n",
            "Epoch [3/4], Step [1800/30000], Loss: 4.9940\n",
            "Epoch [3/4], Step [1900/30000], Loss: 4.3133\n",
            "Epoch [3/4], Step [2000/30000], Loss: 4.0167\n",
            "Epoch [3/4], Step [2100/30000], Loss: 4.6561\n",
            "Epoch [3/4], Step [2200/30000], Loss: 4.4075\n",
            "Epoch [3/4], Step [2300/30000], Loss: 4.0636\n",
            "Epoch [3/4], Step [2400/30000], Loss: 4.1078\n",
            "Epoch [3/4], Step [2500/30000], Loss: 4.3479\n",
            "Epoch [3/4], Step [2600/30000], Loss: 3.9745\n",
            "Epoch [3/4], Step [2700/30000], Loss: 4.3053\n",
            "Epoch [3/4], Step [2800/30000], Loss: 4.7234\n",
            "Epoch [3/4], Step [2900/30000], Loss: 4.8831\n",
            "Epoch [3/4], Step [3000/30000], Loss: 3.6931\n",
            "Epoch [3/4], Step [3100/30000], Loss: 5.0573\n",
            "Epoch [3/4], Step [3200/30000], Loss: 4.0134\n",
            "Epoch [3/4], Step [3300/30000], Loss: 4.9281\n",
            "Epoch [3/4], Step [3400/30000], Loss: 4.9876\n",
            "Epoch [3/4], Step [3500/30000], Loss: 5.1184\n",
            "Epoch [3/4], Step [3600/30000], Loss: 5.3021\n",
            "Epoch [3/4], Step [3700/30000], Loss: 4.1145\n",
            "Epoch [3/4], Step [3800/30000], Loss: 4.6667\n",
            "Epoch [3/4], Step [3900/30000], Loss: 4.0618\n",
            "Epoch [3/4], Step [4000/30000], Loss: 4.6150\n",
            "Epoch [3/4], Step [4100/30000], Loss: 3.9414\n",
            "Epoch [3/4], Step [4200/30000], Loss: 4.6869\n",
            "Epoch [3/4], Step [4300/30000], Loss: 4.0542\n",
            "Epoch [3/4], Step [4400/30000], Loss: 3.8769\n",
            "Epoch [3/4], Step [4500/30000], Loss: 4.3746\n",
            "Epoch [3/4], Step [4600/30000], Loss: 4.7277\n",
            "Epoch [3/4], Step [4700/30000], Loss: 7.2804\n",
            "Epoch [3/4], Step [4800/30000], Loss: 4.0132\n",
            "Epoch [3/4], Step [4900/30000], Loss: 4.3928\n",
            "Epoch [3/4], Step [5000/30000], Loss: 4.9437\n",
            "Epoch [3/4], Step [5100/30000], Loss: 4.4289\n",
            "Epoch [3/4], Step [5200/30000], Loss: 4.6981\n",
            "Epoch [3/4], Step [5300/30000], Loss: 5.1929\n",
            "Epoch [3/4], Step [5400/30000], Loss: 5.3445\n",
            "Epoch [3/4], Step [5500/30000], Loss: 5.4294\n",
            "Epoch [3/4], Step [5600/30000], Loss: 4.3184\n",
            "Epoch [3/4], Step [5700/30000], Loss: 3.2491\n",
            "Epoch [3/4], Step [5800/30000], Loss: 4.0291\n",
            "Epoch [3/4], Step [5900/30000], Loss: 5.0736\n",
            "Epoch [3/4], Step [6000/30000], Loss: 4.8098\n",
            "Epoch [3/4], Step [6100/30000], Loss: 4.7780\n",
            "Epoch [3/4], Step [6200/30000], Loss: 4.6620\n",
            "Epoch [3/4], Step [6300/30000], Loss: 4.4200\n",
            "Epoch [3/4], Step [6400/30000], Loss: 5.7329\n",
            "Epoch [3/4], Step [6500/30000], Loss: 5.0696\n",
            "Epoch [3/4], Step [6600/30000], Loss: 4.5108\n",
            "Epoch [3/4], Step [6700/30000], Loss: 5.5083\n",
            "Epoch [3/4], Step [6800/30000], Loss: 4.0287\n",
            "Epoch [3/4], Step [6900/30000], Loss: 4.8953\n",
            "Epoch [3/4], Step [7000/30000], Loss: 4.5429\n",
            "Epoch [3/4], Step [7100/30000], Loss: 4.1492\n",
            "Epoch [3/4], Step [7200/30000], Loss: 4.6555\n",
            "Epoch [3/4], Step [7300/30000], Loss: 6.0163\n",
            "Epoch [3/4], Step [7400/30000], Loss: 4.4298\n",
            "Epoch [3/4], Step [7500/30000], Loss: 3.5713\n",
            "Epoch [3/4], Step [7600/30000], Loss: 4.3068\n",
            "Epoch [3/4], Step [7700/30000], Loss: 4.6430\n",
            "Epoch [3/4], Step [7800/30000], Loss: 5.0796\n",
            "Epoch [3/4], Step [7900/30000], Loss: 4.6811\n",
            "Epoch [3/4], Step [8000/30000], Loss: 4.3800\n",
            "Epoch [3/4], Step [8100/30000], Loss: 4.4321\n",
            "Epoch [3/4], Step [8200/30000], Loss: 4.1560\n",
            "Epoch [3/4], Step [8300/30000], Loss: 5.5689\n",
            "Epoch [3/4], Step [8400/30000], Loss: 4.0020\n",
            "Epoch [3/4], Step [8500/30000], Loss: 3.7974\n",
            "Epoch [3/4], Step [8600/30000], Loss: 4.5767\n",
            "Epoch [3/4], Step [8700/30000], Loss: 4.3119\n",
            "Epoch [3/4], Step [8800/30000], Loss: 4.4445\n",
            "Epoch [3/4], Step [8900/30000], Loss: 4.6502\n",
            "Epoch [3/4], Step [9000/30000], Loss: 4.5191\n",
            "Epoch [3/4], Step [9100/30000], Loss: 4.1699\n",
            "Epoch [3/4], Step [9200/30000], Loss: 5.1999\n",
            "Epoch [3/4], Step [9300/30000], Loss: 4.7551\n",
            "Epoch [3/4], Step [9400/30000], Loss: 4.7905\n",
            "Epoch [3/4], Step [9500/30000], Loss: 2.9958\n",
            "Epoch [3/4], Step [9600/30000], Loss: 4.7319\n",
            "Epoch [3/4], Step [9700/30000], Loss: 5.0332\n",
            "Epoch [3/4], Step [9800/30000], Loss: 4.5743\n",
            "Epoch [3/4], Step [9900/30000], Loss: 5.1030\n",
            "Epoch [3/4], Step [10000/30000], Loss: 4.5943\n",
            "Epoch [3/4], Step [10100/30000], Loss: 4.8517\n",
            "Epoch [3/4], Step [10200/30000], Loss: 5.3999\n",
            "Epoch [3/4], Step [10300/30000], Loss: 5.2238\n",
            "Epoch [3/4], Step [10400/30000], Loss: 5.0893\n",
            "Epoch [3/4], Step [10500/30000], Loss: 5.8499\n",
            "Epoch [3/4], Step [10600/30000], Loss: 4.2334\n",
            "Epoch [3/4], Step [10700/30000], Loss: 4.9532\n",
            "Epoch [3/4], Step [10800/30000], Loss: 4.5559\n",
            "Epoch [3/4], Step [10900/30000], Loss: 4.4881\n",
            "Epoch [3/4], Step [11000/30000], Loss: 5.0076\n",
            "Epoch [3/4], Step [11100/30000], Loss: 5.0105\n",
            "Epoch [3/4], Step [11200/30000], Loss: 4.3457\n",
            "Epoch [3/4], Step [11300/30000], Loss: 4.5445\n",
            "Epoch [3/4], Step [11400/30000], Loss: 3.3355\n",
            "Epoch [3/4], Step [11500/30000], Loss: 4.5418\n",
            "Epoch [3/4], Step [11600/30000], Loss: 4.9279\n",
            "Epoch [3/4], Step [11700/30000], Loss: 5.6945\n",
            "Epoch [3/4], Step [11800/30000], Loss: 4.9388\n",
            "Epoch [3/4], Step [11900/30000], Loss: 5.3266\n",
            "Epoch [3/4], Step [12000/30000], Loss: 4.1061\n",
            "Epoch [3/4], Step [12100/30000], Loss: 4.1432\n",
            "Epoch [3/4], Step [12200/30000], Loss: 6.3252\n",
            "Epoch [3/4], Step [12300/30000], Loss: 4.0658\n",
            "Epoch [3/4], Step [12400/30000], Loss: 4.4393\n",
            "Epoch [3/4], Step [12500/30000], Loss: 4.8715\n",
            "Epoch [3/4], Step [12600/30000], Loss: 4.6170\n",
            "Epoch [3/4], Step [12700/30000], Loss: 4.7935\n",
            "Epoch [3/4], Step [12800/30000], Loss: 4.0928\n",
            "Epoch [3/4], Step [12900/30000], Loss: 4.4862\n",
            "Epoch [3/4], Step [13000/30000], Loss: 3.8204\n",
            "Epoch [3/4], Step [13100/30000], Loss: 4.2294\n",
            "Epoch [3/4], Step [13200/30000], Loss: 5.4588\n",
            "Epoch [3/4], Step [13300/30000], Loss: 5.6985\n",
            "Epoch [3/4], Step [13400/30000], Loss: 4.3981\n",
            "Epoch [3/4], Step [13500/30000], Loss: 4.5040\n",
            "Epoch [3/4], Step [13600/30000], Loss: 4.7227\n",
            "Epoch [3/4], Step [13700/30000], Loss: 4.5208\n",
            "Epoch [3/4], Step [13800/30000], Loss: 4.2297\n",
            "Epoch [3/4], Step [13900/30000], Loss: 3.9694\n",
            "Epoch [3/4], Step [14000/30000], Loss: 4.7484\n",
            "Epoch [3/4], Step [14100/30000], Loss: 4.9144\n",
            "Epoch [3/4], Step [14200/30000], Loss: 6.9266\n",
            "Epoch [3/4], Step [14300/30000], Loss: 4.2524\n",
            "Epoch [3/4], Step [14400/30000], Loss: 6.6040\n",
            "Epoch [3/4], Step [14500/30000], Loss: 5.0267\n",
            "Epoch [3/4], Step [14600/30000], Loss: 5.0992\n",
            "Epoch [3/4], Step [14700/30000], Loss: 3.5737\n",
            "Epoch [3/4], Step [14800/30000], Loss: 4.5677\n",
            "Epoch [3/4], Step [14900/30000], Loss: 4.4775\n",
            "Epoch [3/4], Step [15000/30000], Loss: 5.7625\n",
            "Epoch [3/4], Step [15100/30000], Loss: 5.6799\n",
            "Epoch [3/4], Step [15200/30000], Loss: 3.2187\n",
            "Epoch [3/4], Step [15300/30000], Loss: 4.1833\n",
            "Epoch [3/4], Step [15400/30000], Loss: 4.5273\n",
            "Epoch [3/4], Step [15500/30000], Loss: 5.2501\n",
            "Epoch [3/4], Step [15600/30000], Loss: 3.7879\n",
            "Epoch [3/4], Step [15700/30000], Loss: 3.9299\n",
            "Epoch [3/4], Step [15800/30000], Loss: 4.9356\n",
            "Epoch [3/4], Step [15900/30000], Loss: 4.6216\n",
            "Epoch [3/4], Step [16000/30000], Loss: 4.8673\n",
            "Epoch [3/4], Step [16100/30000], Loss: 3.4112\n",
            "Epoch [3/4], Step [16200/30000], Loss: 3.8813\n",
            "Epoch [3/4], Step [16300/30000], Loss: 5.1514\n",
            "Epoch [3/4], Step [16400/30000], Loss: 5.0521\n",
            "Epoch [3/4], Step [16500/30000], Loss: 3.5398\n",
            "Epoch [3/4], Step [16600/30000], Loss: 4.1484\n",
            "Epoch [3/4], Step [16700/30000], Loss: 5.9388\n",
            "Epoch [3/4], Step [16800/30000], Loss: 4.4428\n",
            "Epoch [3/4], Step [16900/30000], Loss: 4.9242\n",
            "Epoch [3/4], Step [17000/30000], Loss: 5.1740\n",
            "Epoch [3/4], Step [17100/30000], Loss: 3.8279\n",
            "Epoch [3/4], Step [17200/30000], Loss: 4.7618\n",
            "Epoch [3/4], Step [17300/30000], Loss: 5.2246\n",
            "Epoch [3/4], Step [17400/30000], Loss: 4.4172\n",
            "Epoch [3/4], Step [17500/30000], Loss: 4.0858\n",
            "Epoch [3/4], Step [17600/30000], Loss: 4.1408\n",
            "Epoch [3/4], Step [17700/30000], Loss: 4.1221\n",
            "Epoch [3/4], Step [17800/30000], Loss: 3.5800\n",
            "Epoch [3/4], Step [17900/30000], Loss: 5.5668\n",
            "Epoch [3/4], Step [18000/30000], Loss: 5.5939\n",
            "Epoch [3/4], Step [18100/30000], Loss: 5.2710\n",
            "Epoch [3/4], Step [18200/30000], Loss: 4.7889\n",
            "Epoch [3/4], Step [18300/30000], Loss: 5.2360\n",
            "Epoch [3/4], Step [18400/30000], Loss: 4.7312\n",
            "Epoch [3/4], Step [18500/30000], Loss: 4.1965\n",
            "Epoch [3/4], Step [18600/30000], Loss: 4.6023\n",
            "Epoch [3/4], Step [18700/30000], Loss: 5.2066\n",
            "Epoch [3/4], Step [18800/30000], Loss: 4.7211\n",
            "Epoch [3/4], Step [18900/30000], Loss: 4.0763\n",
            "Epoch [3/4], Step [19000/30000], Loss: 4.1979\n",
            "Epoch [3/4], Step [19100/30000], Loss: 4.8445\n",
            "Epoch [3/4], Step [19200/30000], Loss: 4.6681\n",
            "Epoch [3/4], Step [19300/30000], Loss: 4.6176\n",
            "Epoch [3/4], Step [19400/30000], Loss: 4.8536\n",
            "Epoch [3/4], Step [19500/30000], Loss: 5.2397\n",
            "Epoch [3/4], Step [19600/30000], Loss: 3.9036\n",
            "Epoch [3/4], Step [19700/30000], Loss: 4.0463\n",
            "Epoch [3/4], Step [19800/30000], Loss: 4.8825\n",
            "Epoch [3/4], Step [19900/30000], Loss: 4.1572\n",
            "Epoch [3/4], Step [20000/30000], Loss: 4.9524\n",
            "Epoch [3/4], Step [20100/30000], Loss: 4.0417\n",
            "Epoch [3/4], Step [20200/30000], Loss: 4.3003\n",
            "Epoch [3/4], Step [20300/30000], Loss: 4.0801\n",
            "Epoch [3/4], Step [20400/30000], Loss: 3.6608\n",
            "Epoch [3/4], Step [20500/30000], Loss: 4.5701\n",
            "Epoch [3/4], Step [20600/30000], Loss: 5.4702\n",
            "Epoch [3/4], Step [20700/30000], Loss: 4.8593\n",
            "Epoch [3/4], Step [20800/30000], Loss: 5.6253\n",
            "Epoch [3/4], Step [20900/30000], Loss: 4.2469\n",
            "Epoch [3/4], Step [21000/30000], Loss: 4.3217\n",
            "Epoch [3/4], Step [21100/30000], Loss: 3.6701\n",
            "Epoch [3/4], Step [21200/30000], Loss: 5.3466\n",
            "Epoch [3/4], Step [21300/30000], Loss: 4.9960\n",
            "Epoch [3/4], Step [21400/30000], Loss: 5.4221\n",
            "Epoch [3/4], Step [21500/30000], Loss: 4.8559\n",
            "Epoch [3/4], Step [21600/30000], Loss: 5.0498\n",
            "Epoch [3/4], Step [21700/30000], Loss: 4.2021\n",
            "Epoch [3/4], Step [21800/30000], Loss: 2.6698\n",
            "Epoch [3/4], Step [21900/30000], Loss: 5.1311\n",
            "Epoch [3/4], Step [22000/30000], Loss: 4.3910\n",
            "Epoch [3/4], Step [22100/30000], Loss: 5.0561\n",
            "Epoch [3/4], Step [22200/30000], Loss: 4.9685\n",
            "Epoch [3/4], Step [22300/30000], Loss: 4.5473\n",
            "Epoch [3/4], Step [22400/30000], Loss: 6.0607\n",
            "Epoch [3/4], Step [22500/30000], Loss: 4.9583\n",
            "Epoch [3/4], Step [22600/30000], Loss: 4.0036\n",
            "Epoch [3/4], Step [22700/30000], Loss: 4.4265\n",
            "Epoch [3/4], Step [22800/30000], Loss: 4.4049\n",
            "Epoch [3/4], Step [22900/30000], Loss: 4.2169\n",
            "Epoch [3/4], Step [23000/30000], Loss: 4.4388\n",
            "Epoch [3/4], Step [23100/30000], Loss: 4.3998\n",
            "Epoch [3/4], Step [23200/30000], Loss: 4.4789\n",
            "Epoch [3/4], Step [23300/30000], Loss: 4.2481\n",
            "Epoch [3/4], Step [23400/30000], Loss: 5.2217\n",
            "Epoch [3/4], Step [23500/30000], Loss: 3.0596\n",
            "Epoch [3/4], Step [23600/30000], Loss: 4.9183\n",
            "Epoch [3/4], Step [23700/30000], Loss: 4.4729\n",
            "Epoch [3/4], Step [23800/30000], Loss: 5.3931\n",
            "Epoch [3/4], Step [23900/30000], Loss: 4.2047\n",
            "Epoch [3/4], Step [24000/30000], Loss: 4.3030\n",
            "Epoch [3/4], Step [24100/30000], Loss: 4.0371\n",
            "Epoch [3/4], Step [24200/30000], Loss: 5.4176\n",
            "Epoch [3/4], Step [24300/30000], Loss: 4.0815\n",
            "Epoch [3/4], Step [24400/30000], Loss: 4.9428\n",
            "Epoch [3/4], Step [24500/30000], Loss: 4.1620\n",
            "Epoch [3/4], Step [24600/30000], Loss: 4.6667\n",
            "Epoch [3/4], Step [24700/30000], Loss: 3.9453\n",
            "Epoch [3/4], Step [24800/30000], Loss: 4.2294\n",
            "Epoch [3/4], Step [24900/30000], Loss: 4.5347\n",
            "Epoch [3/4], Step [25000/30000], Loss: 5.0715\n",
            "Epoch [3/4], Step [25100/30000], Loss: 4.2852\n",
            "Epoch [3/4], Step [25200/30000], Loss: 4.7620\n",
            "Epoch [3/4], Step [25300/30000], Loss: 4.5770\n",
            "Epoch [3/4], Step [25400/30000], Loss: 5.7254\n",
            "Epoch [3/4], Step [25500/30000], Loss: 4.5351\n",
            "Epoch [3/4], Step [25600/30000], Loss: 5.2064\n",
            "Epoch [3/4], Step [25700/30000], Loss: 5.4659\n",
            "Epoch [3/4], Step [25800/30000], Loss: 4.2529\n",
            "Epoch [3/4], Step [25900/30000], Loss: 5.9495\n",
            "Epoch [3/4], Step [26000/30000], Loss: 3.5784\n",
            "Epoch [3/4], Step [26100/30000], Loss: 4.8482\n",
            "Epoch [3/4], Step [26200/30000], Loss: 4.7400\n",
            "Epoch [3/4], Step [26300/30000], Loss: 4.2943\n",
            "Epoch [3/4], Step [26400/30000], Loss: 4.0577\n",
            "Epoch [3/4], Step [26500/30000], Loss: 5.4279\n",
            "Epoch [3/4], Step [26600/30000], Loss: 4.2829\n",
            "Epoch [3/4], Step [26700/30000], Loss: 3.8340\n",
            "Epoch [3/4], Step [26800/30000], Loss: 4.1389\n",
            "Epoch [3/4], Step [26900/30000], Loss: 4.4513\n",
            "Epoch [3/4], Step [27000/30000], Loss: 4.4224\n",
            "Epoch [3/4], Step [27100/30000], Loss: 4.1124\n",
            "Epoch [3/4], Step [27200/30000], Loss: 4.1184\n",
            "Epoch [3/4], Step [27300/30000], Loss: 4.8685\n",
            "Epoch [3/4], Step [27400/30000], Loss: 4.4624\n",
            "Epoch [3/4], Step [27500/30000], Loss: 4.0688\n",
            "Epoch [3/4], Step [27600/30000], Loss: 5.6540\n",
            "Epoch [3/4], Step [27700/30000], Loss: 3.6600\n",
            "Epoch [3/4], Step [27800/30000], Loss: 4.7586\n",
            "Epoch [3/4], Step [27900/30000], Loss: 4.3370\n",
            "Epoch [3/4], Step [28000/30000], Loss: 5.0098\n",
            "Epoch [3/4], Step [28100/30000], Loss: 4.2013\n",
            "Epoch [3/4], Step [28200/30000], Loss: 4.1799\n",
            "Epoch [3/4], Step [28300/30000], Loss: 4.1963\n",
            "Epoch [3/4], Step [28400/30000], Loss: 4.3385\n",
            "Epoch [3/4], Step [28500/30000], Loss: 4.7219\n",
            "Epoch [3/4], Step [28600/30000], Loss: 4.4739\n",
            "Epoch [3/4], Step [28700/30000], Loss: 4.9370\n",
            "Epoch [3/4], Step [28800/30000], Loss: 4.8615\n",
            "Epoch [3/4], Step [28900/30000], Loss: 4.9292\n",
            "Epoch [3/4], Step [29000/30000], Loss: 5.7938\n",
            "Epoch [3/4], Step [29100/30000], Loss: 5.5338\n",
            "Epoch [3/4], Step [29200/30000], Loss: 4.7589\n",
            "Epoch [3/4], Step [29300/30000], Loss: 5.1311\n",
            "Epoch [3/4], Step [29400/30000], Loss: 3.6646\n",
            "Epoch [3/4], Step [29500/30000], Loss: 4.4788\n",
            "Epoch [3/4], Step [29600/30000], Loss: 4.5706\n",
            "Epoch [3/4], Step [29700/30000], Loss: 4.4175\n",
            "Epoch [3/4], Step [29800/30000], Loss: 4.5868\n",
            "Epoch [3/4], Step [29900/30000], Loss: 4.6319\n",
            "Epoch [3/4], Step [30000/30000], Loss: 4.4460\n",
            "Epoch [4/4], Step [100/30000], Loss: 5.1444\n",
            "Epoch [4/4], Step [200/30000], Loss: 5.3404\n",
            "Epoch [4/4], Step [300/30000], Loss: 3.9381\n",
            "Epoch [4/4], Step [400/30000], Loss: 4.0425\n",
            "Epoch [4/4], Step [500/30000], Loss: 4.9812\n",
            "Epoch [4/4], Step [600/30000], Loss: 4.4103\n",
            "Epoch [4/4], Step [700/30000], Loss: 3.5825\n",
            "Epoch [4/4], Step [800/30000], Loss: 3.5175\n",
            "Epoch [4/4], Step [900/30000], Loss: 4.9515\n",
            "Epoch [4/4], Step [1000/30000], Loss: 4.9400\n",
            "Epoch [4/4], Step [1100/30000], Loss: 4.2817\n",
            "Epoch [4/4], Step [1200/30000], Loss: 3.7850\n",
            "Epoch [4/4], Step [1300/30000], Loss: 5.9735\n",
            "Epoch [4/4], Step [1400/30000], Loss: 6.4053\n",
            "Epoch [4/4], Step [1500/30000], Loss: 5.2188\n",
            "Epoch [4/4], Step [1600/30000], Loss: 4.6528\n",
            "Epoch [4/4], Step [1700/30000], Loss: 3.4026\n",
            "Epoch [4/4], Step [1800/30000], Loss: 4.7335\n",
            "Epoch [4/4], Step [1900/30000], Loss: 3.7592\n",
            "Epoch [4/4], Step [2000/30000], Loss: 3.6996\n",
            "Epoch [4/4], Step [2100/30000], Loss: 4.5441\n",
            "Epoch [4/4], Step [2200/30000], Loss: 4.8061\n",
            "Epoch [4/4], Step [2300/30000], Loss: 4.1170\n",
            "Epoch [4/4], Step [2400/30000], Loss: 3.8354\n",
            "Epoch [4/4], Step [2500/30000], Loss: 4.4134\n",
            "Epoch [4/4], Step [2600/30000], Loss: 3.0876\n",
            "Epoch [4/4], Step [2700/30000], Loss: 4.5514\n",
            "Epoch [4/4], Step [2800/30000], Loss: 4.6136\n",
            "Epoch [4/4], Step [2900/30000], Loss: 5.0909\n",
            "Epoch [4/4], Step [3000/30000], Loss: 3.3191\n",
            "Epoch [4/4], Step [3100/30000], Loss: 5.1310\n",
            "Epoch [4/4], Step [3200/30000], Loss: 3.7606\n",
            "Epoch [4/4], Step [3300/30000], Loss: 4.7221\n",
            "Epoch [4/4], Step [3400/30000], Loss: 4.6197\n",
            "Epoch [4/4], Step [3500/30000], Loss: 5.8551\n",
            "Epoch [4/4], Step [3600/30000], Loss: 4.3363\n",
            "Epoch [4/4], Step [3700/30000], Loss: 4.6435\n",
            "Epoch [4/4], Step [3800/30000], Loss: 3.9663\n",
            "Epoch [4/4], Step [3900/30000], Loss: 3.9051\n",
            "Epoch [4/4], Step [4000/30000], Loss: 4.6560\n",
            "Epoch [4/4], Step [4100/30000], Loss: 3.2353\n",
            "Epoch [4/4], Step [4200/30000], Loss: 5.1294\n",
            "Epoch [4/4], Step [4300/30000], Loss: 3.8811\n",
            "Epoch [4/4], Step [4400/30000], Loss: 3.7943\n",
            "Epoch [4/4], Step [4500/30000], Loss: 4.4886\n",
            "Epoch [4/4], Step [4600/30000], Loss: 4.6576\n",
            "Epoch [4/4], Step [4700/30000], Loss: 5.7035\n",
            "Epoch [4/4], Step [4800/30000], Loss: 3.7831\n",
            "Epoch [4/4], Step [4900/30000], Loss: 4.2914\n",
            "Epoch [4/4], Step [5000/30000], Loss: 4.7687\n",
            "Epoch [4/4], Step [5100/30000], Loss: 3.7677\n",
            "Epoch [4/4], Step [5200/30000], Loss: 4.5555\n",
            "Epoch [4/4], Step [5300/30000], Loss: 4.7872\n",
            "Epoch [4/4], Step [5400/30000], Loss: 4.6049\n",
            "Epoch [4/4], Step [5500/30000], Loss: 5.0407\n",
            "Epoch [4/4], Step [5600/30000], Loss: 4.3818\n",
            "Epoch [4/4], Step [5700/30000], Loss: 2.2795\n",
            "Epoch [4/4], Step [5800/30000], Loss: 4.0371\n",
            "Epoch [4/4], Step [5900/30000], Loss: 4.4799\n",
            "Epoch [4/4], Step [6000/30000], Loss: 4.6768\n",
            "Epoch [4/4], Step [6100/30000], Loss: 4.3471\n",
            "Epoch [4/4], Step [6200/30000], Loss: 4.6159\n",
            "Epoch [4/4], Step [6300/30000], Loss: 4.4116\n",
            "Epoch [4/4], Step [6400/30000], Loss: 6.0545\n",
            "Epoch [4/4], Step [6500/30000], Loss: 5.7874\n",
            "Epoch [4/4], Step [6600/30000], Loss: 3.9468\n",
            "Epoch [4/4], Step [6700/30000], Loss: 5.7582\n",
            "Epoch [4/4], Step [6800/30000], Loss: 3.8701\n",
            "Epoch [4/4], Step [6900/30000], Loss: 4.5328\n",
            "Epoch [4/4], Step [7000/30000], Loss: 4.2466\n",
            "Epoch [4/4], Step [7100/30000], Loss: 3.6863\n",
            "Epoch [4/4], Step [7200/30000], Loss: 4.3237\n",
            "Epoch [4/4], Step [7300/30000], Loss: 5.9624\n",
            "Epoch [4/4], Step [7400/30000], Loss: 4.4976\n",
            "Epoch [4/4], Step [7500/30000], Loss: 3.1025\n",
            "Epoch [4/4], Step [7600/30000], Loss: 4.3805\n",
            "Epoch [4/4], Step [7700/30000], Loss: 5.4229\n",
            "Epoch [4/4], Step [7800/30000], Loss: 4.7042\n",
            "Epoch [4/4], Step [7900/30000], Loss: 4.3224\n",
            "Epoch [4/4], Step [8000/30000], Loss: 4.5433\n",
            "Epoch [4/4], Step [8100/30000], Loss: 4.2028\n",
            "Epoch [4/4], Step [8200/30000], Loss: 4.4774\n",
            "Epoch [4/4], Step [8300/30000], Loss: 4.6828\n",
            "Epoch [4/4], Step [8400/30000], Loss: 4.0674\n",
            "Epoch [4/4], Step [8500/30000], Loss: 3.7877\n",
            "Epoch [4/4], Step [8600/30000], Loss: 4.0042\n",
            "Epoch [4/4], Step [8700/30000], Loss: 3.9016\n",
            "Epoch [4/4], Step [8800/30000], Loss: 4.4205\n",
            "Epoch [4/4], Step [8900/30000], Loss: 4.4913\n",
            "Epoch [4/4], Step [9000/30000], Loss: 4.2403\n",
            "Epoch [4/4], Step [9100/30000], Loss: 3.9374\n",
            "Epoch [4/4], Step [9200/30000], Loss: 4.8829\n",
            "Epoch [4/4], Step [9300/30000], Loss: 5.1282\n",
            "Epoch [4/4], Step [9400/30000], Loss: 4.5710\n",
            "Epoch [4/4], Step [9500/30000], Loss: 3.0323\n",
            "Epoch [4/4], Step [9600/30000], Loss: 4.6498\n",
            "Epoch [4/4], Step [9700/30000], Loss: 4.9700\n",
            "Epoch [4/4], Step [9800/30000], Loss: 4.7047\n",
            "Epoch [4/4], Step [9900/30000], Loss: 4.6198\n",
            "Epoch [4/4], Step [10000/30000], Loss: 4.2848\n",
            "Epoch [4/4], Step [10100/30000], Loss: 4.7949\n",
            "Epoch [4/4], Step [10200/30000], Loss: 5.0783\n",
            "Epoch [4/4], Step [10300/30000], Loss: 4.8357\n",
            "Epoch [4/4], Step [10400/30000], Loss: 6.7654\n",
            "Epoch [4/4], Step [10500/30000], Loss: 5.2140\n",
            "Epoch [4/4], Step [10600/30000], Loss: 4.0964\n",
            "Epoch [4/4], Step [10700/30000], Loss: 4.9674\n",
            "Epoch [4/4], Step [10800/30000], Loss: 5.2997\n",
            "Epoch [4/4], Step [10900/30000], Loss: 4.4443\n",
            "Epoch [4/4], Step [11000/30000], Loss: 4.8545\n",
            "Epoch [4/4], Step [11100/30000], Loss: 4.9319\n",
            "Epoch [4/4], Step [11200/30000], Loss: 4.1073\n",
            "Epoch [4/4], Step [11300/30000], Loss: 5.0389\n",
            "Epoch [4/4], Step [11400/30000], Loss: 2.9430\n",
            "Epoch [4/4], Step [11500/30000], Loss: 4.4801\n",
            "Epoch [4/4], Step [11600/30000], Loss: 4.7117\n",
            "Epoch [4/4], Step [11700/30000], Loss: 4.8275\n",
            "Epoch [4/4], Step [11800/30000], Loss: 5.2588\n",
            "Epoch [4/4], Step [11900/30000], Loss: 5.6970\n",
            "Epoch [4/4], Step [12000/30000], Loss: 4.2174\n",
            "Epoch [4/4], Step [12100/30000], Loss: 3.7636\n",
            "Epoch [4/4], Step [12200/30000], Loss: 4.8895\n",
            "Epoch [4/4], Step [12300/30000], Loss: 3.8662\n",
            "Epoch [4/4], Step [12400/30000], Loss: 4.2689\n",
            "Epoch [4/4], Step [12500/30000], Loss: 5.5426\n",
            "Epoch [4/4], Step [12600/30000], Loss: 4.9219\n",
            "Epoch [4/4], Step [12700/30000], Loss: 4.6970\n",
            "Epoch [4/4], Step [12800/30000], Loss: 4.2578\n",
            "Epoch [4/4], Step [12900/30000], Loss: 3.8634\n",
            "Epoch [4/4], Step [13000/30000], Loss: 3.6081\n",
            "Epoch [4/4], Step [13100/30000], Loss: 4.1128\n",
            "Epoch [4/4], Step [13200/30000], Loss: 5.4507\n",
            "Epoch [4/4], Step [13300/30000], Loss: 4.8870\n",
            "Epoch [4/4], Step [13400/30000], Loss: 4.1902\n",
            "Epoch [4/4], Step [13500/30000], Loss: 5.3976\n",
            "Epoch [4/4], Step [13600/30000], Loss: 4.5966\n",
            "Epoch [4/4], Step [13700/30000], Loss: 4.8712\n",
            "Epoch [4/4], Step [13800/30000], Loss: 3.8886\n",
            "Epoch [4/4], Step [13900/30000], Loss: 3.5886\n",
            "Epoch [4/4], Step [14000/30000], Loss: 5.0317\n",
            "Epoch [4/4], Step [14100/30000], Loss: 5.4254\n",
            "Epoch [4/4], Step [14200/30000], Loss: 4.9811\n",
            "Epoch [4/4], Step [14300/30000], Loss: 4.7037\n",
            "Epoch [4/4], Step [14400/30000], Loss: 5.2423\n",
            "Epoch [4/4], Step [14500/30000], Loss: 4.9155\n",
            "Epoch [4/4], Step [14600/30000], Loss: 4.6910\n",
            "Epoch [4/4], Step [14700/30000], Loss: 4.3014\n",
            "Epoch [4/4], Step [14800/30000], Loss: 4.7295\n",
            "Epoch [4/4], Step [14900/30000], Loss: 4.3348\n",
            "Epoch [4/4], Step [15000/30000], Loss: 5.1774\n",
            "Epoch [4/4], Step [15100/30000], Loss: 5.3585\n",
            "Epoch [4/4], Step [15200/30000], Loss: 2.9344\n",
            "Epoch [4/4], Step [15300/30000], Loss: 4.2581\n",
            "Epoch [4/4], Step [15400/30000], Loss: 4.8990\n",
            "Epoch [4/4], Step [15500/30000], Loss: 4.8171\n",
            "Epoch [4/4], Step [15600/30000], Loss: 3.7137\n",
            "Epoch [4/4], Step [15700/30000], Loss: 3.5588\n",
            "Epoch [4/4], Step [15800/30000], Loss: 4.9764\n",
            "Epoch [4/4], Step [15900/30000], Loss: 5.3398\n",
            "Epoch [4/4], Step [16000/30000], Loss: 4.5037\n",
            "Epoch [4/4], Step [16100/30000], Loss: 3.5510\n",
            "Epoch [4/4], Step [16200/30000], Loss: 3.3527\n",
            "Epoch [4/4], Step [16300/30000], Loss: 5.1290\n",
            "Epoch [4/4], Step [16400/30000], Loss: 4.7747\n",
            "Epoch [4/4], Step [16500/30000], Loss: 3.3316\n",
            "Epoch [4/4], Step [16600/30000], Loss: 4.0762\n",
            "Epoch [4/4], Step [16700/30000], Loss: 5.0290\n",
            "Epoch [4/4], Step [16800/30000], Loss: 3.8500\n",
            "Epoch [4/4], Step [16900/30000], Loss: 5.0319\n",
            "Epoch [4/4], Step [17000/30000], Loss: 5.0968\n",
            "Epoch [4/4], Step [17100/30000], Loss: 3.9981\n",
            "Epoch [4/4], Step [17200/30000], Loss: 5.5640\n",
            "Epoch [4/4], Step [17300/30000], Loss: 4.8831\n",
            "Epoch [4/4], Step [17400/30000], Loss: 4.2884\n",
            "Epoch [4/4], Step [17500/30000], Loss: 4.1068\n",
            "Epoch [4/4], Step [17600/30000], Loss: 5.1716\n",
            "Epoch [4/4], Step [17700/30000], Loss: 6.3868\n",
            "Epoch [4/4], Step [17800/30000], Loss: 3.6619\n",
            "Epoch [4/4], Step [17900/30000], Loss: 4.7638\n",
            "Epoch [4/4], Step [18000/30000], Loss: 4.7370\n",
            "Epoch [4/4], Step [18100/30000], Loss: 4.8132\n",
            "Epoch [4/4], Step [18200/30000], Loss: 4.7171\n",
            "Epoch [4/4], Step [18300/30000], Loss: 5.6569\n",
            "Epoch [4/4], Step [18400/30000], Loss: 4.3968\n",
            "Epoch [4/4], Step [18500/30000], Loss: 4.1419\n",
            "Epoch [4/4], Step [18600/30000], Loss: 4.8385\n",
            "Epoch [4/4], Step [18700/30000], Loss: 5.1302\n",
            "Epoch [4/4], Step [18800/30000], Loss: 5.3263\n",
            "Epoch [4/4], Step [18900/30000], Loss: 3.8893\n",
            "Epoch [4/4], Step [19000/30000], Loss: 3.9900\n",
            "Epoch [4/4], Step [19100/30000], Loss: 5.0692\n",
            "Epoch [4/4], Step [19200/30000], Loss: 4.5974\n",
            "Epoch [4/4], Step [19300/30000], Loss: 4.6437\n",
            "Epoch [4/4], Step [19400/30000], Loss: 4.8936\n",
            "Epoch [4/4], Step [19500/30000], Loss: 4.9822\n",
            "Epoch [4/4], Step [19600/30000], Loss: 3.9059\n",
            "Epoch [4/4], Step [19700/30000], Loss: 3.8823\n",
            "Epoch [4/4], Step [19800/30000], Loss: 4.8815\n",
            "Epoch [4/4], Step [19900/30000], Loss: 3.9337\n",
            "Epoch [4/4], Step [20000/30000], Loss: 5.6365\n",
            "Epoch [4/4], Step [20100/30000], Loss: 3.8302\n",
            "Epoch [4/4], Step [20200/30000], Loss: 4.4083\n",
            "Epoch [4/4], Step [20300/30000], Loss: 5.0873\n",
            "Epoch [4/4], Step [20400/30000], Loss: 3.5531\n",
            "Epoch [4/4], Step [20500/30000], Loss: 6.4721\n",
            "Epoch [4/4], Step [20600/30000], Loss: 5.0477\n",
            "Epoch [4/4], Step [20700/30000], Loss: 4.7908\n",
            "Epoch [4/4], Step [20800/30000], Loss: 5.0712\n",
            "Epoch [4/4], Step [20900/30000], Loss: 4.2823\n",
            "Epoch [4/4], Step [21000/30000], Loss: 4.0019\n",
            "Epoch [4/4], Step [21100/30000], Loss: 4.1911\n",
            "Epoch [4/4], Step [21200/30000], Loss: 5.1450\n",
            "Epoch [4/4], Step [21300/30000], Loss: 5.3348\n",
            "Epoch [4/4], Step [21400/30000], Loss: 5.0447\n",
            "Epoch [4/4], Step [21500/30000], Loss: 5.0923\n",
            "Epoch [4/4], Step [21600/30000], Loss: 4.8747\n",
            "Epoch [4/4], Step [21700/30000], Loss: 3.9414\n",
            "Epoch [4/4], Step [21800/30000], Loss: 2.3112\n",
            "Epoch [4/4], Step [21900/30000], Loss: 4.9955\n",
            "Epoch [4/4], Step [22000/30000], Loss: 5.0422\n",
            "Epoch [4/4], Step [22100/30000], Loss: 4.9723\n",
            "Epoch [4/4], Step [22200/30000], Loss: 4.9308\n",
            "Epoch [4/4], Step [22300/30000], Loss: 4.5928\n",
            "Epoch [4/4], Step [22400/30000], Loss: 4.3777\n",
            "Epoch [4/4], Step [22500/30000], Loss: 5.7835\n",
            "Epoch [4/4], Step [22600/30000], Loss: 4.3643\n",
            "Epoch [4/4], Step [22700/30000], Loss: 5.6633\n",
            "Epoch [4/4], Step [22800/30000], Loss: 4.3730\n",
            "Epoch [4/4], Step [22900/30000], Loss: 4.0782\n",
            "Epoch [4/4], Step [23000/30000], Loss: 4.6332\n",
            "Epoch [4/4], Step [23100/30000], Loss: 4.3663\n",
            "Epoch [4/4], Step [23200/30000], Loss: 4.5717\n",
            "Epoch [4/4], Step [23300/30000], Loss: 4.0718\n",
            "Epoch [4/4], Step [23400/30000], Loss: 5.1301\n",
            "Epoch [4/4], Step [23500/30000], Loss: 2.9769\n",
            "Epoch [4/4], Step [23600/30000], Loss: 4.8980\n",
            "Epoch [4/4], Step [23700/30000], Loss: 4.4414\n",
            "Epoch [4/4], Step [23800/30000], Loss: 5.1845\n",
            "Epoch [4/4], Step [23900/30000], Loss: 4.6790\n",
            "Epoch [4/4], Step [24000/30000], Loss: 4.3598\n",
            "Epoch [4/4], Step [24100/30000], Loss: 4.0248\n",
            "Epoch [4/4], Step [24200/30000], Loss: 5.7484\n",
            "Epoch [4/4], Step [24300/30000], Loss: 4.0461\n",
            "Epoch [4/4], Step [24400/30000], Loss: 4.7410\n",
            "Epoch [4/4], Step [24500/30000], Loss: 4.2152\n",
            "Epoch [4/4], Step [24600/30000], Loss: 4.5663\n",
            "Epoch [4/4], Step [24700/30000], Loss: 3.8021\n",
            "Epoch [4/4], Step [24800/30000], Loss: 4.1929\n",
            "Epoch [4/4], Step [24900/30000], Loss: 4.6471\n",
            "Epoch [4/4], Step [25000/30000], Loss: 4.9274\n",
            "Epoch [4/4], Step [25100/30000], Loss: 4.2846\n",
            "Epoch [4/4], Step [25200/30000], Loss: 4.5098\n",
            "Epoch [4/4], Step [25300/30000], Loss: 5.3188\n",
            "Epoch [4/4], Step [25400/30000], Loss: 4.9503\n",
            "Epoch [4/4], Step [25500/30000], Loss: 4.7083\n",
            "Epoch [4/4], Step [25600/30000], Loss: 4.7228\n",
            "Epoch [4/4], Step [25700/30000], Loss: 6.6068\n",
            "Epoch [4/4], Step [25800/30000], Loss: 4.1037\n",
            "Epoch [4/4], Step [25900/30000], Loss: 6.1780\n",
            "Epoch [4/4], Step [26000/30000], Loss: 3.9254\n",
            "Epoch [4/4], Step [26100/30000], Loss: 5.0212\n",
            "Epoch [4/4], Step [26200/30000], Loss: 4.5746\n",
            "Epoch [4/4], Step [26300/30000], Loss: 4.5711\n",
            "Epoch [4/4], Step [26400/30000], Loss: 4.9410\n",
            "Epoch [4/4], Step [26500/30000], Loss: 4.2314\n",
            "Epoch [4/4], Step [26600/30000], Loss: 5.1811\n",
            "Epoch [4/4], Step [26700/30000], Loss: 3.6849\n",
            "Epoch [4/4], Step [26800/30000], Loss: 4.1103\n",
            "Epoch [4/4], Step [26900/30000], Loss: 4.3440\n",
            "Epoch [4/4], Step [27000/30000], Loss: 4.5662\n",
            "Epoch [4/4], Step [27100/30000], Loss: 4.5076\n",
            "Epoch [4/4], Step [27200/30000], Loss: 3.9621\n",
            "Epoch [4/4], Step [27300/30000], Loss: 6.0393\n",
            "Epoch [4/4], Step [27400/30000], Loss: 4.5748\n",
            "Epoch [4/4], Step [27500/30000], Loss: 3.9405\n",
            "Epoch [4/4], Step [27600/30000], Loss: 4.8257\n",
            "Epoch [4/4], Step [27700/30000], Loss: 3.3281\n",
            "Epoch [4/4], Step [27800/30000], Loss: 4.5723\n",
            "Epoch [4/4], Step [27900/30000], Loss: 4.3591\n",
            "Epoch [4/4], Step [28000/30000], Loss: 4.5146\n",
            "Epoch [4/4], Step [28100/30000], Loss: 4.0056\n",
            "Epoch [4/4], Step [28200/30000], Loss: 4.0130\n",
            "Epoch [4/4], Step [28300/30000], Loss: 5.7773\n",
            "Epoch [4/4], Step [28400/30000], Loss: 5.2305\n",
            "Epoch [4/4], Step [28500/30000], Loss: 4.5361\n",
            "Epoch [4/4], Step [28600/30000], Loss: 4.5872\n",
            "Epoch [4/4], Step [28700/30000], Loss: 4.8216\n",
            "Epoch [4/4], Step [28800/30000], Loss: 4.7946\n",
            "Epoch [4/4], Step [28900/30000], Loss: 5.0503\n",
            "Epoch [4/4], Step [29000/30000], Loss: 6.1462\n",
            "Epoch [4/4], Step [29100/30000], Loss: 5.1819\n",
            "Epoch [4/4], Step [29200/30000], Loss: 4.6659\n",
            "Epoch [4/4], Step [29300/30000], Loss: 5.0189\n",
            "Epoch [4/4], Step [29400/30000], Loss: 3.2091\n",
            "Epoch [4/4], Step [29500/30000], Loss: 4.3866\n",
            "Epoch [4/4], Step [29600/30000], Loss: 4.7651\n",
            "Epoch [4/4], Step [29700/30000], Loss: 4.3559\n",
            "Epoch [4/4], Step [29800/30000], Loss: 4.5880\n",
            "Epoch [4/4], Step [29900/30000], Loss: 4.7913\n",
            "Epoch [4/4], Step [30000/30000], Loss: 4.4033\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3FrFOP9aEtN"
      },
      "source": [
        "# Predict "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBArQ3oYSw3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "4c36519e-17a4-4fd8-cb8b-ad7dd2b17440"
      },
      "source": [
        "images, labels = next(iter(test_ul_dataloader))\n",
        "imshow(images[0]) # torchvision.utils.make_grid(images[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noD_ancQtarY"
      },
      "source": [
        "# Setup CSV for predictions export\n",
        "df = pd.DataFrame(columns=['# Id', 'Category'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_3icX5sZ2CF"
      },
      "source": [
        "with torch.no_grad():\n",
        "    i = 0\n",
        "    for data in test_ul_dataloader:\n",
        "        images, labels = data\n",
        "        images = data[0].to(device)[None, :]\n",
        "        images = images.permute(1, 0, 2, 3)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        label_predicted = labels_encoder.inverse_transform([predicted.item()])\n",
        "        prediction = str(label_predicted[0])\n",
        "        df.loc[i] = [i, prediction]\n",
        "        i += 1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssnB3f2W5wVS"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgwPKD_p1Vt"
      },
      "source": [
        "# Export CSV for Kaggle\n",
        "from datetime import datetime\n",
        "filename = 'kaggle_g19_{}.csv'.format(datetime.now())\n",
        "df.to_csv(filename, sep=',', float_format='{:36}', index=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpTkvmx3ubFV"
      },
      "source": [
        "# test_df = pd.DataFrame(columns=['# Id', 'Category'])\n",
        "# test_df.loc[0] = [1, '00010001000000000']\n",
        "# test_df.to_csv('testcsv.csv', sep=',', float_format='{:36}', index=False)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdI6FpAu7zs"
      },
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBwQPCZ2wjJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}