{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Felix-Exploration.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/felixsimard/comp551-p3/blob/main/Felix_Exploration.ipynb",
      "authorship_tag": "ABX9TyOaWeclxeQhyMb8gruUF1f2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p3/blob/main/Felix_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9d--OU0IF0S"
      },
      "source": [
        "# Felix's Notebook for exploring the assignment"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yNW2EISINDi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgfx5EC0kW8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3f89ea-419d-428a-ed27-37ceb9f0f214"
      },
      "source": [
        "# Reference: https://www.youtube.com/watch?v=pDdP0TFzsoQ\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3doYI1lAbe9"
      },
      "source": [
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5     # unnormalize\n",
        "    # npimg = img.numpy()\n",
        "    plt.imshow(img)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
        "    plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batbKOaCj6fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba94dc4-c5a9-499a-86e7-376c25cc13a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "def load_data(filename, data_path='/content/drive/MyDrive/P3-COMP551-FALL2021/'):\n",
        "    loaded_pkl = None\n",
        "    try:\n",
        "        pkl_buffered = open(data_path+''+filename,'rb')\n",
        "        loaded_pkl = pickle.load(pkl_buffered)\n",
        "    except Exception as e:\n",
        "        print(\"Error loading data: {}\".format(e))\n",
        "    return loaded_pkl\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuvaFp3CvYH-"
      },
      "source": [
        "# Load data\n",
        "train_l = load_data(\"images_l.pkl\")\n",
        "train_ul = load_data(\"images_ul.pkl\")\n",
        "labels_l = load_data(\"labels_l.pkl\")\n",
        "test_ul = load_data(\"images_test.pkl\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds4PQvPLM4OR"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwO8NOgHkd23"
      },
      "source": [
        "# Reference: https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDFiIo7cRWsN"
      },
      "source": [
        "# Hyper-parameters\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0001"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_X-KfukA00R"
      },
      "source": [
        "# Tensor, Transform, Datasets, Dataloaders\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Tensor\n",
        "train_l_tensor = torch.Tensor(train_l)\n",
        "\n",
        "\n",
        "def labelize(lst):\n",
        "    bin_str = \"\".join(str(int(i)) for i in lst)\n",
        "    return bin_str\n",
        "\n",
        "# Labels logic\n",
        "labels_l_lst = labels_l.tolist()\n",
        "labels_l_lst = [labelize(lst) for lst in labels_l_lst]\n",
        "labels_encoder = preprocessing.LabelEncoder()\n",
        "targets = labels_encoder.fit_transform(labels_l_lst)\n",
        "labels_l_tensor = torch.as_tensor(targets)\n",
        "\n",
        "# labels_l_tensor = torch.argmax(torch.Tensor(labels_l), dim=1) # THIS IS KEY FOR THE LABELS\n",
        "\n",
        "# Datasets\n",
        "train_l_dataset = TensorDataset(train_l_tensor, labels_l_tensor)\n",
        "train_l_dataloader = DataLoader(train_l_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Test\n",
        "test_ul_tensor = torch.Tensor(test_ul)\n",
        "test_labels = torch.Tensor(np.zeros(len(test_ul)))\n",
        "test_ul_dataset = TensorDataset(test_ul_tensor, test_labels)\n",
        "test_ul_dataloader = DataLoader(test_ul_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXuPBNFHvY6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27a6f47-b033-480f-8ffc-9cb78205a1aa"
      },
      "source": [
        "# Define labels\n",
        "# Consider upper case and lower case letters?\n",
        "labels = []\n",
        "for l in range(26):\n",
        "    letter_str = [0.0 for i in range(26)]\n",
        "    letter_str[l] = 1.0\n",
        "    for d in range(10):\n",
        "        digits_str = [0.0 for j in range(10)]\n",
        "        digits_str[d] = 1.0\n",
        "        c = digits_str + letter_str\n",
        "        # c = \"\".join(c_str)\n",
        "        labels.append(c)\n",
        "print(labels[:5])\n",
        "print(len(labels))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooBvvB2DE4Vc"
      },
      "source": [
        "# Implement CONV Net\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=260):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        # self.model = torchvision.models.resnet34(pretrained=False)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 6, 5) # input channel (rgb), output channel, kernel size \n",
        "        self.pool = nn.MaxPool2d(2, 2) # define 2x2 stride for max-pooling\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # input channel size = output channel size of previous conv layer\n",
        "        self.fc1 = nn.Linear(16*11*11, 120) # fully connected layer\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)   \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) # activation function does not change size\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*11*11) # -1 tells PyTorch to infer num batches # flatten tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # no activation at end, softmax included in CrossEntropyLoss\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc2s6YwqN28x",
        "outputId": "2dcde8cb-9489-40c7-ab08-8f8c4e9de2ff"
      },
      "source": [
        "# Test model flow\n",
        "model_test = ConvNet()\n",
        "x = torch.randn(BATCH_SIZE, 1, 56, 56)\n",
        "print(model_test(x).shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 260])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Lrt1pstW72"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-n5-iKHQxc",
        "outputId": "4565f48b-80d9-4f94-f01f-f0da970d5f9b"
      },
      "source": [
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # includes softmax\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "n_total_steps = len(train_l_dataloader)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_l_dataloader, 0):\n",
        "        # get inputs, data is a list of [inputs, labels]\n",
        "        inputs = data[0].to(device)[None, :]\n",
        "        inputs = inputs.permute(1, 0, 2, 3)\n",
        "        labels = data[1].to(device)\n",
        "\n",
        "        # print(\"Inputs:\", inputs.shape)\n",
        "        # print(\"labels:\", labels)\n",
        "        # print(\"labels:\", labels.shape)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 100 == 0: # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "        \n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 0.285\n",
            "[1,   200] loss: 0.278\n",
            "[1,   300] loss: 0.278\n",
            "[1,   400] loss: 0.279\n",
            "[1,   500] loss: 0.278\n",
            "[1,   600] loss: 0.278\n",
            "[1,   700] loss: 0.278\n",
            "[1,   800] loss: 0.278\n",
            "[1,   900] loss: 0.277\n",
            "[1,  1000] loss: 0.277\n",
            "[1,  1100] loss: 0.277\n",
            "[1,  1200] loss: 0.276\n",
            "[1,  1300] loss: 0.276\n",
            "[1,  1400] loss: 0.273\n",
            "[1,  1500] loss: 0.274\n",
            "[1,  1600] loss: 0.273\n",
            "[1,  1700] loss: 0.271\n",
            "[1,  1800] loss: 0.273\n",
            "[1,  1900] loss: 0.268\n",
            "[1,  2000] loss: 0.270\n",
            "[1,  2100] loss: 0.266\n",
            "[1,  2200] loss: 0.265\n",
            "[1,  2300] loss: 0.266\n",
            "[1,  2400] loss: 0.268\n",
            "[1,  2500] loss: 0.265\n",
            "[1,  2600] loss: 0.265\n",
            "[1,  2700] loss: 0.265\n",
            "[1,  2800] loss: 0.266\n",
            "[1,  2900] loss: 0.263\n",
            "[1,  3000] loss: 0.263\n",
            "[1,  3100] loss: 0.267\n",
            "[1,  3200] loss: 0.266\n",
            "[1,  3300] loss: 0.263\n",
            "[1,  3400] loss: 0.261\n",
            "[1,  3500] loss: 0.262\n",
            "[1,  3600] loss: 0.268\n",
            "[1,  3700] loss: 0.268\n",
            "[1,  3800] loss: 0.262\n",
            "[1,  3900] loss: 0.267\n",
            "[1,  4000] loss: 0.265\n",
            "[1,  4100] loss: 0.266\n",
            "[1,  4200] loss: 0.263\n",
            "[1,  4300] loss: 0.262\n",
            "[1,  4400] loss: 0.261\n",
            "[1,  4500] loss: 0.263\n",
            "[1,  4600] loss: 0.262\n",
            "[1,  4700] loss: 0.262\n",
            "[1,  4800] loss: 0.262\n",
            "[1,  4900] loss: 0.257\n",
            "[1,  5000] loss: 0.262\n",
            "[1,  5100] loss: 0.266\n",
            "[1,  5200] loss: 0.260\n",
            "[1,  5300] loss: 0.259\n",
            "[1,  5400] loss: 0.259\n",
            "[1,  5500] loss: 0.262\n",
            "[1,  5600] loss: 0.263\n",
            "[1,  5700] loss: 0.262\n",
            "[1,  5800] loss: 0.261\n",
            "[1,  5900] loss: 0.260\n",
            "[1,  6000] loss: 0.262\n",
            "[1,  6100] loss: 0.262\n",
            "[1,  6200] loss: 0.257\n",
            "[1,  6300] loss: 0.261\n",
            "[1,  6400] loss: 0.259\n",
            "[1,  6500] loss: 0.260\n",
            "[1,  6600] loss: 0.257\n",
            "[1,  6700] loss: 0.261\n",
            "[1,  6800] loss: 0.259\n",
            "[1,  6900] loss: 0.260\n",
            "[1,  7000] loss: 0.258\n",
            "[1,  7100] loss: 0.260\n",
            "[1,  7200] loss: 0.261\n",
            "[1,  7300] loss: 0.259\n",
            "[1,  7400] loss: 0.263\n",
            "[1,  7500] loss: 0.259\n",
            "[1,  7600] loss: 0.260\n",
            "[1,  7700] loss: 0.258\n",
            "[1,  7800] loss: 0.261\n",
            "[1,  7900] loss: 0.257\n",
            "[1,  8000] loss: 0.257\n",
            "[1,  8100] loss: 0.259\n",
            "[1,  8200] loss: 0.262\n",
            "[1,  8300] loss: 0.257\n",
            "[1,  8400] loss: 0.259\n",
            "[1,  8500] loss: 0.257\n",
            "[1,  8600] loss: 0.261\n",
            "[1,  8700] loss: 0.256\n",
            "[1,  8800] loss: 0.258\n",
            "[1,  8900] loss: 0.259\n",
            "[1,  9000] loss: 0.262\n",
            "[1,  9100] loss: 0.257\n",
            "[1,  9200] loss: 0.261\n",
            "[1,  9300] loss: 0.255\n",
            "[1,  9400] loss: 0.257\n",
            "[1,  9500] loss: 0.258\n",
            "[1,  9600] loss: 0.255\n",
            "[1,  9700] loss: 0.259\n",
            "[1,  9800] loss: 0.257\n",
            "[1,  9900] loss: 0.256\n",
            "[1, 10000] loss: 0.255\n",
            "[1, 10100] loss: 0.257\n",
            "[1, 10200] loss: 0.257\n",
            "[1, 10300] loss: 0.254\n",
            "[1, 10400] loss: 0.254\n",
            "[1, 10500] loss: 0.253\n",
            "[1, 10600] loss: 0.258\n",
            "[1, 10700] loss: 0.256\n",
            "[1, 10800] loss: 0.256\n",
            "[1, 10900] loss: 0.258\n",
            "[1, 11000] loss: 0.254\n",
            "[1, 11100] loss: 0.254\n",
            "[1, 11200] loss: 0.255\n",
            "[1, 11300] loss: 0.253\n",
            "[1, 11400] loss: 0.254\n",
            "[1, 11500] loss: 0.257\n",
            "[1, 11600] loss: 0.255\n",
            "[1, 11700] loss: 0.256\n",
            "[1, 11800] loss: 0.256\n",
            "[1, 11900] loss: 0.252\n",
            "[1, 12000] loss: 0.256\n",
            "[1, 12100] loss: 0.252\n",
            "[1, 12200] loss: 0.253\n",
            "[1, 12300] loss: 0.256\n",
            "[1, 12400] loss: 0.256\n",
            "[1, 12500] loss: 0.253\n",
            "[1, 12600] loss: 0.256\n",
            "[1, 12700] loss: 0.255\n",
            "[1, 12800] loss: 0.256\n",
            "[1, 12900] loss: 0.255\n",
            "[1, 13000] loss: 0.251\n",
            "[1, 13100] loss: 0.256\n",
            "[1, 13200] loss: 0.252\n",
            "[1, 13300] loss: 0.251\n",
            "[1, 13400] loss: 0.253\n",
            "[1, 13500] loss: 0.255\n",
            "[1, 13600] loss: 0.254\n",
            "[1, 13700] loss: 0.254\n",
            "[1, 13800] loss: 0.249\n",
            "[1, 13900] loss: 0.254\n",
            "[1, 14000] loss: 0.254\n",
            "[1, 14100] loss: 0.257\n",
            "[1, 14200] loss: 0.254\n",
            "[1, 14300] loss: 0.258\n",
            "[1, 14400] loss: 0.252\n",
            "[1, 14500] loss: 0.255\n",
            "[1, 14600] loss: 0.252\n",
            "[1, 14700] loss: 0.254\n",
            "[1, 14800] loss: 0.249\n",
            "[1, 14900] loss: 0.249\n",
            "[1, 15000] loss: 0.252\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3FrFOP9aEtN"
      },
      "source": [
        "# Predict "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBArQ3oYSw3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "4f5e23a8-963c-4d93-a885-34ee89628cb6"
      },
      "source": [
        "images, labels = next(iter(test_ul_dataloader))\n",
        "imshow(images[0]) # torchvision.utils.make_grid(images[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noD_ancQtarY"
      },
      "source": [
        "# Setup CSV for predictions export\n",
        "df = pd.DataFrame(columns=['# Id', 'Category'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_3icX5sZ2CF"
      },
      "source": [
        "with torch.no_grad():\n",
        "    i = 0\n",
        "    for data in test_ul_dataloader:\n",
        "        images, labels = data\n",
        "        images = data[0].to(device)[None, :]\n",
        "        images = images.permute(1, 0, 2, 3)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        for batch in range(BATCH_SIZE):\n",
        "            label_predicted = labels_encoder.inverse_transform([predicted[batch].item()])\n",
        "            prediction = str(label_predicted[0])\n",
        "            df.loc[i] = [i, prediction]\n",
        "            i += 1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssnB3f2W5wVS"
      },
      "source": [
        "# Helper function to convert bit string to human-readable label\n",
        "import string\n",
        "def convert_bit_string_to_label(bit_str):\n",
        "    alphabet_string = string.ascii_lowercase\n",
        "    letters_lst = list(alphabet_string)\n",
        "    digits_lst = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "    out = []\n",
        "    for i, d in enumerate(bit_str[:10]):\n",
        "        if int(d) == 1:\n",
        "            out.append(digits_lst[i])\n",
        "    for i, l in enumerate(bit_str[10:]):\n",
        "        if int(l) == 1:\n",
        "            out.append(letters_lst[i])\n",
        "    return '-'.join(out)\n",
        "\n",
        "# print(convert_bit_string_to_label('000000001000000000000000010000000000'))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT3ng35WIGGe"
      },
      "source": [
        "# Check out visually the predictions of our model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgwPKD_p1Vt"
      },
      "source": [
        "# Export CSV for Kaggle\n",
        "from datetime import datetime\n",
        "filename = 'kaggle_g19_{}.csv'.format(datetime.now())\n",
        "df.to_csv(filename, sep=',', float_format='{:36}', index=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpTkvmx3ubFV"
      },
      "source": [
        "# test_df = pd.DataFrame(columns=['# Id', 'Category'])\n",
        "# test_df.loc[0] = [1, '00010001000000000']\n",
        "# test_df.to_csv('testcsv.csv', sep=',', float_format='{:36}', index=False)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdI6FpAu7zs"
      },
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBwQPCZ2wjJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}