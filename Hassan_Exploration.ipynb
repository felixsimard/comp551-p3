{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hassan_Exploration.ipynb",
      "provenance": [],
      "mount_file_id": "1PSxie5U7EJ6OA6Sp7zWWm78HCNeW82kb",
      "authorship_tag": "ABX9TyOr8rQS1ovdQCQw7XXOqKTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p3/blob/main/Hassan_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgX1RFRDD5oV"
      },
      "source": [
        "- Notebook to build CNN image classifier\n",
        "- Based on https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ojQ-VPtEH2C"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import drive\n",
        "from typing import List"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlmPTd1MhvSP"
      },
      "source": [
        "## Pickle Data to Numpy NDArray"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4pSpno3Fdvb"
      },
      "source": [
        "# Felix's load data fn\n",
        "# Function to return pickle loaded file in an ndarray\n",
        "def load_data(filename, data_path='/content/drive/MyDrive/data/'):\n",
        "    drive.mount(\"/content/drive\")\n",
        "    loaded_pkl = None\n",
        "    try:\n",
        "        pkl_buffered = open(data_path+''+filename,'rb')\n",
        "        loaded_pkl = pickle.load(pkl_buffered)\n",
        "    except Exception as e:\n",
        "        print(\"Error loading data: {}\".format(e))\n",
        "    return loaded_pkl"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQW7faTuIUvF",
        "outputId": "5a8364e8-83f6-4814-e05f-fa0e556baa67"
      },
      "source": [
        "# loading all data\n",
        "train_features = load_data(\"images_l.pkl\")\n",
        "train_labels = load_data(\"labels_l.pkl\")\n",
        "test = load_data(\"images_test.pkl\")\n",
        "train_unlabelled = load_data(\"images_ul.pkl\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilsd5Pq9JKvR",
        "outputId": "aa31fa23-9d6c-42b0-8805-0b53affad535"
      },
      "source": [
        "print(train_features.shape, train_features[:1])\n",
        "print(train_labels.shape, train_labels[:1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 56, 56) [[[  0.   0.   0. ... 175.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0. 175.   0.]\n",
            "  ...\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]]]\n",
            "(30000, 36) [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnMeMk82d3-r"
      },
      "source": [
        "- `train_features` has 30,000 samples of 56x56 images\n",
        "- `train_labels` labels of the 56x56 images, a 36-bit binary vector\n",
        "- The code block below verifies the image data are all in numpy n-dimensional arrays, `np.ndarray`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmXhhEyf-71",
        "outputId": "162c3eba-2340-417e-aef6-e6631cd61af4"
      },
      "source": [
        "for data in [train_features, train_labels, train_unlabelled, test]:  \n",
        "  print(type(data) is np.ndarray)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQUwa6owksmq"
      },
      "source": [
        "## Tensor DataLoader & Feature Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6r1rnFkvfm"
      },
      "source": [
        "# Transforming the numpy arrays into tensors with the labels\n",
        "# Concatenating datasets to have Tensor([[image_features], label])\n",
        "training = DataLoader(TensorDataset(torch.Tensor(train_features).unsqueeze(1),\n",
        "                                    torch.Tensor(train_labels).unsqueeze(1)))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXd9nIYEsZco"
      },
      "source": [
        "- The classification task calls for classifying an image that contains:\n",
        "1. Characters `A-Z` OR `a-z`\n",
        "2. Numbers `0-9`\n",
        "- Each image will include any combination of 1 lower OR uppercase character and one number\n",
        "- Therefore, the labels will have to include every combination of these characters and numbers:\n",
        "1. 260 different classes: `0-9` AND `A-Z`\n",
        "2. 260 different classes: `0-9` AND `a-z`\n",
        "- A total of 520 `labels`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJxJe91NtFqv",
        "outputId": "2dfe7dec-f6f3-439d-ee69-421c58145812"
      },
      "source": [
        "labels = []\n",
        "\n",
        "# This implementation is from Felix\n",
        "for l in range(26):\n",
        "    lowerC, upperC = [0.0 for i in range(26)], [0.0 for i in range(26)] \n",
        "    lowerC[l], upperC[l] = 1.0, 1.0\n",
        "    for d in range(10):\n",
        "        digits_str = [0.0 for j in range(10)]\n",
        "        digits_str[d] = 1.0\n",
        "        Lc = lowerC + digits_str\n",
        "        Uc = upperC + digits_str\n",
        "        labels.append(Lc)\n",
        "        labels.append(Uc)\n",
        "\n",
        "print(labels[:3], len(labels))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]] 520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwywA9w9iQ7y"
      },
      "source": [
        "## Conv. NN Class (Implementation of VGG11 Deep CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2psXaEpiU2T"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  # Constructor\n",
        "  def __init__(self, in_channels=1, num_classes=520):\n",
        "    super(CNN, self).__init__()         # Access methods in parent class\n",
        "    self.in_channels = in_channels\n",
        "    self.num_classes = num_classes\n",
        "    # convolutional layers \n",
        "    self.conv_layers = nn.Sequential(\n",
        "      nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "      )\n",
        "        # fully connected linear layers\n",
        "    self.linear_layers = nn.Sequential(\n",
        "      nn.Linear(in_features=512, out_features=4096),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(0.5),\n",
        "      nn.Linear(in_features=4096, out_features=512),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(0.5),\n",
        "      nn.Linear(in_features=512, out_features=36)\n",
        "      )\n",
        "    \n",
        "  def forward(self, x):\n",
        "      print(x.shape)\n",
        "      x = self.conv_layers(x)\n",
        "      print(x.shape)\n",
        "      # flatten to prepare for the fully connected layers\n",
        "      x = x.view(1, 512)\n",
        "      print(x.shape)\n",
        "      x = self.linear_layers(x)\n",
        "      print(x.shape)\n",
        "      return x\n",
        "  "
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szvWO_fnGEwh"
      },
      "source": [
        "# Model Loss, Optimization, & Run with CUDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqMYvDABJco_"
      },
      "source": [
        "epochs = 4\n",
        "batch = 8\n",
        "lr = 0.005"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vbYaFIBFx-k"
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "steps = len(training)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "rxKqEp_BTS3N",
        "outputId": "f1756cd4-7ef1-47c8-f437-b2f98d719201"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i, data in enumerate(training, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].squeeze_().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "torch.save(model.state_dict(), './cnn.pth')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 56, 56])\n",
            "torch.Size([1, 512, 1, 1])\n",
            "torch.Size([1, 512])\n",
            "torch.Size([1, 36])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-d8238718df51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1121\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (36)."
          ]
        }
      ]
    }
  ]
}