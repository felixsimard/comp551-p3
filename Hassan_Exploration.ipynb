{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hassan_Exploration.ipynb",
      "provenance": [],
      "mount_file_id": "1PSxie5U7EJ6OA6Sp7zWWm78HCNeW82kb",
      "authorship_tag": "ABX9TyNCvruHg618K1ASfI35UgkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p3/blob/main/Hassan_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ojQ-VPtEH2C"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from google.colab import drive\n",
        "from sklearn import preprocessing\n",
        "from PIL import Image\n",
        "from typing import List\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcUNnH2F0-Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b796d3-e062-43d2-a975-e266784ba558"
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw1G23aC1a13"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5JUILhN2hJ0"
      },
      "source": [
        "* Displays an image given a bit-based input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAE0dPe-1fdj"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(img)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6LLTMkP2qZx"
      },
      "source": [
        "* Class to create datasets with correct dimensionality properties, retreived from StackOverflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkFH277n1kNp"
      },
      "source": [
        "# Reference: https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets=None, transform=None, transform_target=None):\n",
        "        self.data = torch.from_numpy(data).float()\n",
        "        self.targets = torch.from_numpy(targets).float() if targets is not None else None\n",
        "        self.transform = transform\n",
        "        self.transform_target = transform_target\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = np.zeros(36, dtype=float)# self.targets[index]\n",
        "\n",
        "        if self.targets is not None:\n",
        "            y = self.targets[index]\n",
        "        else:\n",
        "            None\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        if self.transform_target:\n",
        "            y = self.transform_target(y)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDY2DCoj20_U"
      },
      "source": [
        "* Export a CSV file in the parent directory given a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNA676-J1pKQ"
      },
      "source": [
        "def makeMyCSV(dataf: pd.DataFrame)->None:\n",
        "  filename = 'kaggle_g19_{}.csv'.format(datetime.now())\n",
        "  dataf.to_csv(filename, sep=',', float_format='{:36}', index=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M2n5aro1_wy"
      },
      "source": [
        "# Felix's load data fn\n",
        "# Function to return pickle loaded file in an ndarray\n",
        "def load_data(filename, data_path='/content/drive/MyDrive/data/'):\n",
        "    drive.mount(\"/content/drive\")\n",
        "    loaded_pkl = None\n",
        "    try:\n",
        "        pkl_buffered = open(data_path+''+filename,'rb')\n",
        "        loaded_pkl = pickle.load(pkl_buffered)\n",
        "    except Exception as e:\n",
        "        print(\"Error loading data: {}\".format(e))\n",
        "    return loaded_pkl"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlW81ckJ2QzT"
      },
      "source": [
        "def get_label_value(labels):\n",
        "  \"\"\"\n",
        "  This function will return a string representing the label of a picture given\n",
        "  the array label as input:\n",
        "  Ex ouput: '1a', '4z' ...\n",
        "  \"\"\"\n",
        "  label_temp = labels.tolist()\n",
        "  label_temp = [int(x) for x in label_temp]\n",
        "  number = label_temp[:10].index(1)\n",
        "  letter = alpha_dict[label_temp[10:].index(1)]\n",
        "\n",
        "  return str(number) + str(letter)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTtNy1VV2UKT"
      },
      "source": [
        "def transform_output(scores):\n",
        "    \"\"\"\n",
        "    Input a Tensor and output will be another Tensor with same dimension but with all elements 0 except two.\n",
        "    Those 2 elements will have value of 1 and will correspond to the models prediction about which letter and number\n",
        "    is in the image.\n",
        "    :param scores:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return_array = []\n",
        "    score_list = scores.tolist()\n",
        "\n",
        "    for score in score_list:\n",
        "        numbers = score[:10]\n",
        "        letters = score[10:]\n",
        "        test = lambda x, max_value : 1 if x >= max_value else 0\n",
        "\n",
        "        new_numbers = [test(x, max(numbers)) for x in numbers]\n",
        "        new_letters = [test(x, max(letters)) for x in letters]\n",
        "\n",
        "        return_array.append(new_numbers + new_letters)\n",
        "\n",
        "    return return_array"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICKOzMRLga-3"
      },
      "source": [
        "def convert_outputs_to_preds(outputs):\n",
        "    preds = np.empty(shape=(len(outputs), 36))\n",
        "    for i, output in enumerate(outputs):\n",
        "        pred = np.zeros(36)\n",
        "        digit_index = np.argmax(output[:11])\n",
        "        letter_index = np.argmax(output[11:]) + 11\n",
        "        pred[digit_index], pred[letter_index] = 1, 1\n",
        "        preds[i] = pred\n",
        "    return preds"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbXQ-mXIgcOC"
      },
      "source": [
        "def correct_digit(pred, label):\n",
        "    return np.array_equal(pred[:11],label[:11])\n",
        "\n",
        "def correct_letter(pred, label):\n",
        "    return np.array_equal(pred[11:],label[11:])\n",
        "\n",
        "def get_accuracy(results):\n",
        "    return round(sum(results) / len(results), 2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlmPTd1MhvSP"
      },
      "source": [
        "## Pickle Data to Numpy NDArray"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQW7faTuIUvF",
        "outputId": "be42d6e9-0565-48af-c3b1-ff28eca32e31"
      },
      "source": [
        "# loading all data\n",
        "train_features = load_data(\"images_l.pkl\")[:, None]\n",
        "train_labels = load_data(\"labels_l.pkl\")\n",
        "test = load_data(\"images_test.pkl\")[:, None]\n",
        "train_unlabelled = load_data(\"images_ul.pkl\")[:, None]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilsd5Pq9JKvR",
        "outputId": "4b0f907f-1f28-4783-ac72-5e374936d494"
      },
      "source": [
        "print(train_features.shape, train_features[:1])\n",
        "print(train_labels.shape, train_labels[:1])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 1, 56, 56) [[[[  0.   0.   0. ... 175.   0.   0.]\n",
            "   [  0.   0.   0. ...   0.   0.   0.]\n",
            "   [  0.   0.   0. ...   0. 175.   0.]\n",
            "   ...\n",
            "   [  0.   0.   0. ...   0.   0.   0.]\n",
            "   [  0.   0.   0. ...   0.   0.   0.]\n",
            "   [  0.   0.   0. ...   0.   0.   0.]]]]\n",
            "(30000, 36) [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnMeMk82d3-r"
      },
      "source": [
        "- `train_features` has 30,000 samples of 56x56 images\n",
        "- `train_labels` labels of the 56x56 images, a 36-bit binary vector\n",
        "- The code block below verifies the image data are all in numpy n-dimensional arrays, `np.ndarray`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmXhhEyf-71",
        "outputId": "6125d8df-7bc4-4c85-ae48-de71182064cd"
      },
      "source": [
        "for data in [train_features, train_labels, train_unlabelled, test]:  \n",
        "  print(type(data) is np.ndarray)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du32maX_5WBs"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePlviMIE5c7F"
      },
      "source": [
        "epochs = 4\n",
        "batch = 8\n",
        "lr = 0.002             # learning rate\n",
        "channels = 1           # Image input channel\n",
        "beta = 0.6             # SGD momentum coefficient\n",
        "train_test_split = 0.3 # 70% training data, 30% validation data\n",
        "flatten_dim = 56 * 56  # image dim = 56 x 56 px"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R33l2H6jI6jx"
      },
      "source": [
        "## Training & Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc8wgGMzagTc"
      },
      "source": [
        "# Data transformation parameters\n",
        "mean = (0.5,)\n",
        "std = (0.5,)\n",
        "Transform = transforms.Compose([transforms.Normalize(mean=mean, std=std)])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsIDoeOyaRCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5403d2e8-433c-4f25-8571-edf3dc0febcd"
      },
      "source": [
        "split_index = math.floor(len(train_labels)*train_test_split)\n",
        "\n",
        "full_train_l = train_features\n",
        "val_l = train_features[:split_index]\n",
        "train_l = train_features[split_index:]\n",
        "\n",
        "full_train_labels_l = train_labels\n",
        "val_labels_l = train_labels[:split_index]\n",
        "train_labels_l = train_labels[split_index:]\n",
        "\n",
        "print(full_train_l.shape, full_train_labels_l.shape)\n",
        "print(train_l.shape, train_labels_l.shape)\n",
        "print(val_l.shape, val_labels_l.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 1, 56, 56) (30000, 36)\n",
            "(21000, 1, 56, 56) (21000, 36)\n",
            "(9000, 1, 56, 56) (9000, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQUwa6owksmq"
      },
      "source": [
        "## Tensor DataLoader & Feature Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkylO2dBofPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9b05ff-5089-4e8b-e81c-8c6354a79f1d"
      },
      "source": [
        "# DataLoaders\n",
        "all_training = DataLoader(MyDataset(full_train_l, full_train_labels_l, \n",
        "                                                   transform=Transform), shuffle=True, batch_size=batch)\n",
        "training = DataLoader(MyDataset(train_l, train_labels_l, \n",
        "                                                   transform=Transform), shuffle=True, batch_size=batch)\n",
        "validation = DataLoader(MyDataset(val_l, val_labels_l, \n",
        "                                                   transform=Transform), shuffle=True)\n",
        "\n",
        "# Test set for Kaggle\n",
        "test_labels_ul = np.zeros(len(test))\n",
        "testing = DataLoader(MyDataset(test, test_labels_ul, \n",
        "                                                    transform=Transform), batch_size=batch, shuffle=False)\n",
        "\n",
        "print(len(all_training)*batch,len(training)*batch, len(testing)*batch)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000 21000 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXd9nIYEsZco"
      },
      "source": [
        "- The classification task calls for classifying an image that contains:\n",
        "1. Characters `A-Z` OR `a-z`\n",
        "2. Numbers `0-9`\n",
        "- Each image will include any combination of 1 lower OR uppercase character and one number\n",
        "- Therefore, the labels will have to include every combination of these characters and numbers:\n",
        "1. 260 different classes: `0-9` AND `A-Z`\n",
        "2. 260 different classes: `0-9` AND `a-z`\n",
        "- A total of 520 `labels`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwywA9w9iQ7y"
      },
      "source": [
        "## Conv. NN Class (Implementation of VGG11 Deep CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2psXaEpiU2T"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  # Constructor\n",
        "  def __init__(self, in_channels=1, num_classes=36):\n",
        "    super(CNN, self).__init__()         # Access methods in parent class\n",
        "    self.in_channels = in_channels\n",
        "    self.num_classes = num_classes\n",
        "    # convolutional layers \n",
        "    self.conv_layers = nn.Sequential(\n",
        "      nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "      )\n",
        "        # fully connected linear layers\n",
        "    self.linear_layers = nn.Sequential(\n",
        "      nn.Linear(in_features=512, out_features=4096),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(0.5),\n",
        "      nn.Linear(in_features=4096, out_features=4096),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(0.5),\n",
        "      nn.Linear(in_features=4096, out_features=self.num_classes)\n",
        "      )\n",
        "    \n",
        "  def forward(self, x):\n",
        "      x = self.conv_layers(x)\n",
        "      # flatten to prepare for the fully connected layers\n",
        "      x = x.view(x.size(0),-1)\n",
        "      x = self.linear_layers(x)\n",
        "      return x\n",
        "  "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szvWO_fnGEwh"
      },
      "source": [
        "# Model Training with CUDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vbYaFIBFx-k"
      },
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=beta)\n",
        "steps = len(training)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXj5ZWNZctNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4666be-41e8-42a7-a74d-06ec71338eba"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  model.train()\n",
        "  for i, (inputs, targets) in enumerate(training):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs,targets)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  \n",
        "    if i % 100 == 99:    # print every 100 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "      running_loss = 0.0"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 0.448\n",
            "[1,   200] loss: 0.230\n",
            "[1,   300] loss: 0.223\n",
            "[1,   400] loss: 0.221\n",
            "[1,   500] loss: 0.219\n",
            "[1,   600] loss: 0.220\n",
            "[1,   700] loss: 0.217\n",
            "[1,   800] loss: 0.218\n",
            "[1,   900] loss: 0.218\n",
            "[1,  1000] loss: 0.217\n",
            "[1,  1100] loss: 0.215\n",
            "[1,  1200] loss: 0.217\n",
            "[1,  1300] loss: 0.215\n",
            "[1,  1400] loss: 0.215\n",
            "[1,  1500] loss: 0.215\n",
            "[1,  1600] loss: 0.215\n",
            "[1,  1700] loss: 0.214\n",
            "[1,  1800] loss: 0.214\n",
            "[1,  1900] loss: 0.213\n",
            "[1,  2000] loss: 0.213\n",
            "[1,  2100] loss: 0.215\n",
            "[1,  2200] loss: 0.213\n",
            "[1,  2300] loss: 0.214\n",
            "[1,  2400] loss: 0.213\n",
            "[1,  2500] loss: 0.213\n",
            "[1,  2600] loss: 0.213\n",
            "[2,   100] loss: 0.213\n",
            "[2,   200] loss: 0.212\n",
            "[2,   300] loss: 0.213\n",
            "[2,   400] loss: 0.213\n",
            "[2,   500] loss: 0.213\n",
            "[2,   600] loss: 0.212\n",
            "[2,   700] loss: 0.213\n",
            "[2,   800] loss: 0.213\n",
            "[2,   900] loss: 0.213\n",
            "[2,  1000] loss: 0.213\n",
            "[2,  1100] loss: 0.213\n",
            "[2,  1200] loss: 0.212\n",
            "[2,  1300] loss: 0.212\n",
            "[2,  1400] loss: 0.212\n",
            "[2,  1500] loss: 0.212\n",
            "[2,  1600] loss: 0.212\n",
            "[2,  1700] loss: 0.212\n",
            "[2,  1800] loss: 0.212\n",
            "[2,  1900] loss: 0.211\n",
            "[2,  2000] loss: 0.212\n",
            "[2,  2100] loss: 0.212\n",
            "[2,  2200] loss: 0.212\n",
            "[2,  2300] loss: 0.212\n",
            "[2,  2400] loss: 0.212\n",
            "[2,  2500] loss: 0.212\n",
            "[2,  2600] loss: 0.212\n",
            "[3,   100] loss: 0.212\n",
            "[3,   200] loss: 0.212\n",
            "[3,   300] loss: 0.211\n",
            "[3,   400] loss: 0.211\n",
            "[3,   500] loss: 0.211\n",
            "[3,   600] loss: 0.211\n",
            "[3,   700] loss: 0.211\n",
            "[3,   800] loss: 0.212\n",
            "[3,   900] loss: 0.212\n",
            "[3,  1000] loss: 0.211\n",
            "[3,  1100] loss: 0.212\n",
            "[3,  1200] loss: 0.211\n",
            "[3,  1300] loss: 0.211\n",
            "[3,  1400] loss: 0.211\n",
            "[3,  1500] loss: 0.211\n",
            "[3,  1600] loss: 0.211\n",
            "[3,  1700] loss: 0.211\n",
            "[3,  1800] loss: 0.211\n",
            "[3,  1900] loss: 0.212\n",
            "[3,  2000] loss: 0.210\n",
            "[3,  2100] loss: 0.210\n",
            "[3,  2200] loss: 0.211\n",
            "[3,  2300] loss: 0.211\n",
            "[3,  2400] loss: 0.211\n",
            "[3,  2500] loss: 0.211\n",
            "[3,  2600] loss: 0.211\n",
            "[4,   100] loss: 0.210\n",
            "[4,   200] loss: 0.211\n",
            "[4,   300] loss: 0.211\n",
            "[4,   400] loss: 0.211\n",
            "[4,   500] loss: 0.211\n",
            "[4,   600] loss: 0.211\n",
            "[4,   700] loss: 0.211\n",
            "[4,   800] loss: 0.211\n",
            "[4,   900] loss: 0.211\n",
            "[4,  1000] loss: 0.210\n",
            "[4,  1100] loss: 0.211\n",
            "[4,  1200] loss: 0.210\n",
            "[4,  1300] loss: 0.210\n",
            "[4,  1400] loss: 0.211\n",
            "[4,  1500] loss: 0.210\n",
            "[4,  1600] loss: 0.211\n",
            "[4,  1700] loss: 0.211\n",
            "[4,  1800] loss: 0.211\n",
            "[4,  1900] loss: 0.210\n",
            "[4,  2000] loss: 0.211\n",
            "[4,  2100] loss: 0.211\n",
            "[4,  2200] loss: 0.211\n",
            "[4,  2300] loss: 0.210\n",
            "[4,  2400] loss: 0.210\n",
            "[4,  2500] loss: 0.210\n",
            "[4,  2600] loss: 0.211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcb-8EKNiPHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee44a1d-b8b4-48b5-dbf8-15d34e569915"
      },
      "source": [
        "# Update accuracy metric on validation set\n",
        "validation_acc = []\n",
        "model.eval()\n",
        "digit_results = []\n",
        "letter_results = []\n",
        "for i, data in enumerate(validation):\n",
        "    val_inputs, val_labels = data\n",
        "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "    outputs = model(val_inputs)\n",
        "    val_labels = val_labels.cpu().detach().numpy()\n",
        "    preds = convert_outputs_to_preds(outputs.cpu().detach().numpy())\n",
        "    digit_results.append(correct_digit(preds[0], val_labels[0]))\n",
        "    letter_results.append(correct_letter(preds[0], val_labels[0]))\n",
        "digit_accuracy = get_accuracy(digit_results)\n",
        "letter_accuracy = get_accuracy(letter_results)\n",
        "total_accuracy = get_accuracy(digit_results and letter_results)\n",
        "validation_acc.append((digit_accuracy, letter_accuracy, total_accuracy))\n",
        "\n",
        "# Check validation accuracy\n",
        "for i, val_accuracy in enumerate(validation_acc):\n",
        "  print('Epoch = {}, Total Acc = {}, Digit Acc = {}, Letter Acc = {}'.format(i+1, val_accuracy[2], val_accuracy[0], val_accuracy[1]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch = 1, Total Acc = 0.04, Digit Acc = 0.1, Letter Acc = 0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2h5Dzgct_vp"
      },
      "source": [
        "df = pd.DataFrame(columns=['# Id', 'Category'])\n",
        "#device = torch.device('cpu')\n",
        "with torch.no_grad():\n",
        "    i = 0\n",
        "    for data in test_ul_dataloader:\n",
        "        inputs = inputs.to(device) \n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions = transform_output(outputs)\n",
        "        for pred in predictions:\n",
        "            label = ''.join(str(x) for x in pred)\n",
        "            df.loc[i] = [i, label]\n",
        "            i += 1\n",
        "\n",
        "df = df.iloc[:15001]\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_qou1LlgJuo"
      },
      "source": [
        "makeMyCSV(df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}